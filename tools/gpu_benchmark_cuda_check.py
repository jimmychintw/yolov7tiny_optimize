#!/usr/bin/env python3
"""
GPU/CUDA ç’°å¢ƒæª¢æŸ¥å·¥å…· - ç¢ºä¿çœŸçš„åœ¨ä½¿ç”¨ GPU
"""

import torch
import sys
import subprocess

def check_cuda_environment():
    """åš´æ ¼æª¢æŸ¥ CUDA ç’°å¢ƒ"""
    print("=" * 60)
    print("ğŸ” GPU/CUDA ç’°å¢ƒåš´æ ¼æª¢æŸ¥")
    print("=" * 60)
    
    # 1. æª¢æŸ¥ PyTorch CUDA
    print("\n1ï¸âƒ£ PyTorch CUDA æª¢æŸ¥:")
    cuda_available = torch.cuda.is_available()
    print(f"   CUDA å¯ç”¨: {cuda_available}")
    
    if not cuda_available:
        print("   âŒ éŒ¯èª¤: PyTorch ç„¡æ³•ä½¿ç”¨ CUDA!")
        print("   å¯èƒ½åŸå› :")
        print("   - PyTorch å®‰è£ç‰ˆæœ¬ä¸æ”¯æ´ CUDA")
        print("   - CUDA é©…å‹•ç‰ˆæœ¬ä¸ç›¸å®¹")
        print("   - æ²’æœ‰ NVIDIA GPU")
        return False
    
    print(f"   PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"   PyTorch CUDA ç‰ˆæœ¬: {torch.version.cuda}")
    print(f"   CUDNN ç‰ˆæœ¬: {torch.backends.cudnn.version()}")
    
    # 2. æª¢æŸ¥ GPU è³‡è¨Š
    print("\n2ï¸âƒ£ GPU ç¡¬é«”æª¢æŸ¥:")
    device_count = torch.cuda.device_count()
    print(f"   GPU æ•¸é‡: {device_count}")
    
    if device_count == 0:
        print("   âŒ éŒ¯èª¤: æ‰¾ä¸åˆ°ä»»ä½• GPU!")
        return False
    
    for i in range(device_count):
        props = torch.cuda.get_device_properties(i)
        print(f"   GPU {i}: {props.name}")
        print(f"      è¨˜æ†¶é«”: {props.total_memory / 1024**3:.1f} GB")
        print(f"      è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}")
    
    # 3. nvidia-smi æª¢æŸ¥
    print("\n3ï¸âƒ£ nvidia-smi æª¢æŸ¥:")
    try:
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,driver_version,memory.total',
                               '--format=csv,noheader'], 
                              capture_output=True, text=True, check=True)
        print("   ç³»çµ± GPU è³‡è¨Š:")
        for line in result.stdout.strip().split('\n'):
            print(f"   - {line}")
    except Exception as e:
        print(f"   âš ï¸ nvidia-smi åŸ·è¡Œå¤±æ•—: {e}")
    
    # 4. å¯¦éš› GPU æ¸¬è©¦
    print("\n4ï¸âƒ£ å¯¦éš› GPU é‹ç®—æ¸¬è©¦:")
    try:
        # å»ºç«‹æ¸¬è©¦å¼µé‡
        device = torch.device('cuda:0')
        x = torch.randn(1000, 1000).to(device)
        y = torch.randn(1000, 1000).to(device)
        
        # åŸ·è¡ŒçŸ©é™£é‹ç®—
        torch.cuda.synchronize()
        z = torch.matmul(x, y)
        torch.cuda.synchronize()
        
        print(f"   âœ… GPU é‹ç®—æˆåŠŸ")
        print(f"   æ¸¬è©¦è£ç½®: {device}")
        print(f"   è¨˜æ†¶é«”å·²ä½¿ç”¨: {torch.cuda.memory_allocated(device) / 1024**2:.1f} MB")
        
    except Exception as e:
        print(f"   âŒ GPU é‹ç®—å¤±æ•—: {e}")
        return False
    
    # 5. ç‰ˆæœ¬ç›¸å®¹æ€§æª¢æŸ¥
    print("\n5ï¸âƒ£ ç‰ˆæœ¬ç›¸å®¹æ€§:")
    try:
        # å–å¾—ç³»çµ± CUDA ç‰ˆæœ¬
        result = subprocess.run(['nvcc', '--version'], 
                              capture_output=True, text=True)
        if result.returncode == 0:
            for line in result.stdout.split('\n'):
                if 'release' in line:
                    print(f"   ç³»çµ± CUDA: {line.strip()}")
        
        print(f"   PyTorch CUDA: {torch.version.cuda}")
        
        # æª¢æŸ¥ç‰ˆæœ¬åŒ¹é…
        if torch.version.cuda:
            pytorch_cuda = float(torch.version.cuda.split('.')[0] + '.' + torch.version.cuda.split('.')[1])
            print(f"   ç‰ˆæœ¬æª¢æŸ¥: PyTorch CUDA {pytorch_cuda}")
            
            if pytorch_cuda < 11.8:
                print("   âš ï¸ è­¦å‘Š: H100 å»ºè­°ä½¿ç”¨ CUDA 11.8 æˆ–æ›´é«˜ç‰ˆæœ¬")
        
    except Exception as e:
        print(f"   âš ï¸ ç„¡æ³•æª¢æŸ¥ nvcc: {e}")
    
    print("\n" + "=" * 60)
    print("âœ… GPU/CUDA ç’°å¢ƒæª¢æŸ¥å®Œæˆ - å¯ä»¥ä½¿ç”¨ GPU!")
    print("=" * 60)
    return True

def main():
    """ä¸»ç¨‹å¼"""
    success = check_cuda_environment()
    
    if not success:
        print("\nâŒ GPU ç’°å¢ƒæœ‰å•é¡Œï¼Œè«‹ä¿®å¾©å¾Œå†åŸ·è¡Œæ¸¬è©¦!")
        print("\nå»ºè­°è§£æ±ºæ–¹æ¡ˆ:")
        print("1. é‡æ–°å®‰è£æ­£ç¢ºç‰ˆæœ¬çš„ PyTorch:")
        print("   pip install torch==2.0.1+cu118 --index-url https://download.pytorch.org/whl/cu118")
        print("2. ç¢ºèª NVIDIA é©…å‹•æ­£ç¢ºå®‰è£:")
        print("   nvidia-smi")
        print("3. ç¢ºèª CUDA å·¥å…·åŒ…å®‰è£:")
        print("   nvcc --version")
        sys.exit(1)
    else:
        print("\nâœ… å¯ä»¥åŸ·è¡Œ GPU åŸºæº–æ¸¬è©¦!")
        print("   python tools/gpu_benchmark.py")

if __name__ == "__main__":
    main()