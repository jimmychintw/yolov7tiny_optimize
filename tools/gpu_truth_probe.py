#!/usr/bin/env python3
"""
GPU çœŸç›¸æ¢é‡ - ç¢ºèªç’°å¢ƒèƒ½çœŸæ­£ä½¿ç”¨ GPU ä¸”è¢« nvidia-smi æª¢æ¸¬åˆ°
è¨­è¨ˆç†å¿µï¼šä¸èƒ½éœé»˜é€€å› CPUï¼Œæœƒè®“ GPU å¿™ 10-15 ç§’çš„æœ€å°ç¨‹å¼
"""

import torch
import time
import argparse
import sys

def gpu_truth_probe(seconds=15, size=8192, dtype='bf16'):
    """GPU çœŸç›¸æ¢é‡ - ç¢ºä¿ GPU çœŸçš„åœ¨å·¥ä½œ"""
    print("=" * 60)
    print("ğŸ”¬ GPU çœŸç›¸æ¢é‡å•Ÿå‹•")
    print("=" * 60)
    
    # 1. åš´æ ¼æª¢æŸ¥ CUDA
    if not torch.cuda.is_available():
        print("âŒ è‡´å‘½éŒ¯èª¤: CUDA ä¸å¯ç”¨!")
        sys.exit(1)
    
    device = torch.device('cuda:0')
    gpu_name = torch.cuda.get_device_name(0)
    print(f"ğŸ¯ ç›®æ¨™ GPU: {gpu_name}")
    print(f"ğŸ”§ çŸ©é™£å¤§å°: {size}x{size}")
    print(f"â±ï¸  æ¸¬è©¦æ™‚é•·: {seconds} ç§’")
    print(f"ğŸ”¢ æ•¸æ“šé¡å‹: {dtype}")
    
    # 2. è¨­å®šæ•¸æ“šé¡å‹
    if dtype == 'bf16':
        torch_dtype = torch.bfloat16
        autocast_dtype = torch.bfloat16
    elif dtype == 'fp16':
        torch_dtype = torch.float16  
        autocast_dtype = torch.float16
    else:
        torch_dtype = torch.float32
        autocast_dtype = torch.float32
    
    # 3. å•Ÿç”¨æœ€ä½³åŒ–
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    torch.backends.cudnn.benchmark = True
    
    # 4. å‰µå»ºå¤§çŸ©é™£ç¢ºä¿ GPU å¿™ç¢Œ
    print(f"\nğŸ“¦ å‰µå»º {size}x{size} çŸ©é™£...")
    try:
        # ä½¿ç”¨æŒ‡å®šçš„æ•¸æ“šé¡å‹
        x = torch.randn(size, size, dtype=torch_dtype, device=device)
        y = torch.randn(size, size, dtype=torch_dtype, device=device)
        
        memory_gb = (x.element_size() * x.nelement() + y.element_size() * y.nelement()) / 1024**3
        print(f"ğŸ“Š GPU è¨˜æ†¶é«”ä½¿ç”¨: {memory_gb:.2f} GB")
        
        # å¼·åˆ¶é©—è­‰åœ¨ CUDA ä¸Š
        assert x.is_cuda and y.is_cuda, "âŒ çŸ©é™£ä¸åœ¨ CUDA ä¸Š"
        
    except RuntimeError as e:
        if "out of memory" in str(e):
            print(f"âš ï¸  è¨˜æ†¶é«”ä¸è¶³ï¼Œå˜—è©¦è¼ƒå°çŸ©é™£ ({size//2}x{size//2})")
            size = size // 2
            x = torch.randn(size, size, dtype=torch_dtype, device=device)
            y = torch.randn(size, size, dtype=torch_dtype, device=device)
        else:
            raise e
    
    # 5. é–‹å§‹å£“åŠ›æ¸¬è©¦
    print(f"\nğŸ”¥ é–‹å§‹ {seconds} ç§’å£“åŠ›æ¸¬è©¦...")
    print("ğŸ’¡ è«‹åŒæ™‚åŸ·è¡Œ: watch -n 0.5 nvidia-smi")
    print("-" * 60)
    
    start_time = time.time()
    iteration = 0
    total_iterations = 0
    
    while time.time() - start_time < seconds:
        # ä½¿ç”¨ CUDA Events ç²¾ç¢ºè¨ˆæ™‚
        start_event = torch.cuda.Event(enable_timing=True)
        end_event = torch.cuda.Event(enable_timing=True)
        
        start_event.record()
        
        # å¯†é›†çŸ©é™£é‹ç®—
        with torch.cuda.amp.autocast(dtype=autocast_dtype):
            z = torch.matmul(x, y)
            # é¡å¤–é‹ç®—ç¢ºä¿ GPU çœŸçš„å¿™ç¢Œ
            z = torch.matmul(z, x)
            z = z + torch.randn_like(z) * 0.1
            
        # è®“ GPU å¿™å¾—å¤ ä¹…ï¼Œnvidia-smi æ‰çœ‹å¾—åˆ°
        torch.cuda._sleep(100_000_000)  # ç´„ 100ms
        
        end_event.record()
        torch.cuda.synchronize()
        
        iteration += 1
        total_iterations += 1
        
        # æ¯ç§’å ±å‘Šä¸€æ¬¡
        if iteration >= 3:  # ç´„ 1 ç§’
            elapsed_ms = start_event.elapsed_time(end_event)
            gpu_memory = torch.cuda.memory_allocated() / 1024**2
            print(f"è¿­ä»£ {total_iterations:3d}: {elapsed_ms:.1f}ms/iter | GPUè¨˜æ†¶é«”: {gpu_memory:.0f}MB | å·²é‹è¡Œ: {time.time() - start_time:.1f}s")
            iteration = 0
    
    total_time = time.time() - start_time
    print("-" * 60)
    print(f"âœ… æ¸¬è©¦å®Œæˆ!")
    print(f"ğŸ“Š ç¸½è¿­ä»£: {total_iterations}")
    print(f"â±ï¸  ç¸½æ™‚é–“: {total_time:.2f} ç§’")
    print(f"ğŸš€ å¹³å‡é€Ÿåº¦: {total_iterations/total_time:.2f} æ¬¡/ç§’")
    
    # 6. æœ€çµ‚é©—è­‰
    final_memory = torch.cuda.memory_allocated() / 1024**2
    print(f"ğŸ’¾ æœ€çµ‚ GPU è¨˜æ†¶é«”: {final_memory:.1f} MB")
    
    if final_memory < 100:
        print("âš ï¸  è­¦å‘Š: GPU è¨˜æ†¶é«”ä½¿ç”¨éä½ï¼Œå¯èƒ½æœªçœŸæ­£ä½¿ç”¨ GPU")
        return False
    
    print("\nğŸ¯ è¨ºæ–·çµè«–:")
    print("   å¦‚æœ nvidia-smi é¡¯ç¤º:")
    print("   âœ… GPU ä½¿ç”¨ç‡ > 80% â†’ ç’°å¢ƒæ­£å¸¸")
    print("   âœ… çœ‹åˆ° python é€²ç¨‹ â†’ é€²ç¨‹å¯è¦‹")
    print("   âŒ GPU ä½¿ç”¨ç‡ < 10% â†’ å¯èƒ½æœ‰å•é¡Œ")
    print("   âŒ æ²’æœ‰é€²ç¨‹ â†’ å®¹å™¨æ¬Šé™é™åˆ¶")
    
    return True

def main():
    parser = argparse.ArgumentParser(description="GPU çœŸç›¸æ¢é‡")
    parser.add_argument("--seconds", type=int, default=15, help="æ¸¬è©¦æ™‚é•·ï¼ˆç§’ï¼‰")
    parser.add_argument("--size", type=int, default=8192, help="çŸ©é™£å¤§å°")
    parser.add_argument("--dtype", choices=['fp32', 'fp16', 'bf16'], default='bf16', help="æ•¸æ“šé¡å‹")
    
    args = parser.parse_args()
    
    success = gpu_truth_probe(args.seconds, args.size, args.dtype)
    
    if success:
        print("\nâœ… GPU ç’°å¢ƒæª¢æŸ¥é€šéï¼Œå¯ä»¥åŸ·è¡ŒåŸºæº–æ¸¬è©¦")
        print("   python tools/gpu_benchmark.py")
    else:
        print("\nâŒ GPU ç’°å¢ƒå¯èƒ½æœ‰å•é¡Œï¼Œè«‹æª¢æŸ¥")
        sys.exit(1)

if __name__ == "__main__":
    main()