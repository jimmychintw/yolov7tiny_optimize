#!/usr/bin/env python3
"""
GPU æ•ˆèƒ½æ¸¬è©¦è…³æœ¬ v2.0 - æ“¬çœŸå£“åŠ›æ¸¬è©¦
æ¸¬è©¦ä¸åŒ batch size ä¸‹çš„è¨“ç·´é€Ÿåº¦å’Œè¨˜æ†¶é«”ä½¿ç”¨ï¼ŒåŒ…å«çœŸå¯¦è³‡æ–™è¼‰å…¥å’Œå®Œæ•´ loss è¨ˆç®—
"""

import torch
import torch.nn as nn
import time
import psutil
import yaml
import argparse
from pathlib import Path
import sys
import os
import math
import GPUtil
import threading
from tqdm import tqdm

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent))

from models.yolo import Model
from utils.general import check_img_size, check_dataset
from utils.torch_utils import select_device
from utils.datasets import create_dataloader
from utils.loss import ComputeLoss

class GPUBenchmark:
    def __init__(self, config_file="configs/gpu_configs.yaml"):
        self.config_file = Path(config_file)
        self.load_config()
        self.device = select_device('0')
        self.results = {}
        self.monitoring_active = False
        self.gpu_stats = []
        
    def load_config(self):
        """è¼‰å…¥ GPU é…ç½®"""
        with open(self.config_file, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
    
    def start_gpu_monitoring(self):
        """å•Ÿå‹• GPU ç›£æ§åŸ·è¡Œç·’"""
        self.monitoring_active = True
        self.gpu_stats = []
        
        def monitor():
            while self.monitoring_active:
                try:
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu = gpus[0]
                        self.gpu_stats.append({
                            'timestamp': time.time(),
                            'utilization': gpu.load * 100,
                            'memory_used': gpu.memoryUsed,
                            'memory_total': gpu.memoryTotal,
                            'temperature': gpu.temperature
                        })
                except:
                    pass
                time.sleep(0.5)
        
        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.monitor_thread.start()
    
    def stop_gpu_monitoring(self):
        """åœæ­¢ GPU ç›£æ§"""
        self.monitoring_active = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=1.0)
    
    def detect_gpu(self):
        """è‡ªå‹•åµæ¸¬ GPU å‹è™Ÿ"""
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            print(f"åµæ¸¬åˆ° GPU: {gpu_name}")
            
            # ç°¡åŒ–çš„ GPU é¡å‹åˆ¤æ–·
            if "4090" in gpu_name:
                return "RTX4090"
            elif "5090" in gpu_name:
                return "RTX5090" 
            elif "H100" in gpu_name:
                return "H100"
            elif "B200" in gpu_name:
                return "B200"
            else:
                return "Unknown"
        return None
    
    def create_model(self):
        """å»ºç«‹æ¸¬è©¦ç”¨æ¨¡å‹"""
        cfg = "cfg/training/yolov7-tiny.yaml"
        model = Model(cfg, ch=3, nc=80, anchors=None).to(self.device)
        
        # è¼‰å…¥è¶…åƒæ•¸ä»¥æ”¯æ´ ComputeLoss
        import yaml
        with open("data/hyp.scratch.tiny.yaml", 'r') as f:
            hyp = yaml.safe_load(f)
        model.hyp = hyp  # æ·»åŠ  hyp å±¬æ€§
        
        return model
    
    def find_max_batch_size(self, model, dataloader, compute_loss, start_batch=512, max_batch=4096):
        """äºŒåˆ†æœå°‹æ³•æ‰¾å‡ºæœ€å¤§å¯ç”¨ batch size"""
        print(f"\nğŸ” å°‹æ‰¾æœ€å¤§ batch size (å¾ {start_batch} é–‹å§‹)")
        
        def test_batch_size(batch_size):
            try:
                torch.cuda.empty_cache()
                dummy_input = torch.randn(batch_size, 3, 320, 320).to(self.device)
                dummy_targets = self.create_realistic_targets(batch_size)
                
                with torch.cuda.amp.autocast():
                    outputs = model(dummy_input)
                    loss, _ = compute_loss(outputs, dummy_targets)
                    loss.backward()
                    model.zero_grad()
                
                del dummy_input, dummy_targets, outputs, loss
                torch.cuda.empty_cache()
                return True
            except RuntimeError as e:
                if "out of memory" in str(e):
                    torch.cuda.empty_cache()
                    return False
                raise e
        
        # äºŒåˆ†æœå°‹
        low, high = start_batch, max_batch
        max_successful = start_batch
        
        while low <= high:
            mid = (low + high) // 2
            print(f"  æ¸¬è©¦ batch size: {mid}...", end=" ")
            
            if test_batch_size(mid):
                print("âœ… æˆåŠŸ")
                max_successful = mid
                low = mid + 1
            else:
                print("âŒ OOM")
                high = mid - 1
        
        print(f"ğŸ¯ æ‰¾åˆ°æœ€å¤§ batch size: {max_successful}")
        return max_successful

    def benchmark_batch_size_realistic(self, model, batch_sizes, img_size=320, test_levels=['light', 'medium', 'heavy']):
        """æ“¬çœŸæ¸¬è©¦ä¸åŒ batch size çš„æ•ˆèƒ½"""
        results = {}
        
        # å»ºç«‹çœŸå¯¦è³‡æ–™è¼‰å…¥å™¨ (å°æ‰¹é‡ç”¨æ–¼æ¸¬è©¦)
        print("ğŸ“‚ æº–å‚™çœŸå¯¦è³‡æ–™è¼‰å…¥å™¨...")
        try:
            # å»ºç«‹å‡çš„ opt ç‰©ä»¶ä»¥æ»¿è¶³ create_dataloader éœ€æ±‚
            class FakeOpt:
                single_cls = False
                rect = False
                cache_images = False
                image_weights = False
                quad = False
            
            opt = FakeOpt()
            
            dataloader = create_dataloader(
                path='../coco/val2017.txt',  # ä½¿ç”¨é©—è­‰é›†è·¯å¾‘
                imgsz=img_size,
                batch_size=32,  # å°æ‰¹é‡ç”¨æ–¼æ¡æ¨£
                stride=32,
                opt=opt,
                hyp={'lr0': 0.01},  # ç°¡å–®çš„ hyp åƒæ•¸
                augment=False,
                cache=False,
                pad=0.0,
                rect=False,
                rank=-1,
                world_size=1,
                workers=2,
                image_weights=False,
                quad=False,
                prefix=''
            )[0]
            print("âœ… è³‡æ–™è¼‰å…¥å™¨æº–å‚™å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  è³‡æ–™è¼‰å…¥å™¨å¤±æ•—ï¼Œä½¿ç”¨æ¨¡æ“¬è³‡æ–™: {e}")
            dataloader = None
        
        # å»ºç«‹ loss è¨ˆç®—å™¨
        compute_loss = ComputeLoss(model)
        
        for batch_size in batch_sizes:
            print(f"\nğŸ§ª æ¸¬è©¦ batch_size: {batch_size}")
            batch_results = {}
            
            for level in test_levels:
                try:
                    torch.cuda.empty_cache()
                    
                    # æ ¹æ“šæ¸¬è©¦ç´šåˆ¥è¨­å®šè¿­ä»£æ¬¡æ•¸
                    iterations = {
                        'light': 20,    # è¼•é‡: 20 æ¬¡
                        'medium': 100,  # ä¸­ç­‰: 100 æ¬¡  
                        'heavy': 200    # é‡åº¦: 200 æ¬¡
                    }[level]
                    
                    print(f"  ğŸ“Š {level.upper()} æ¸¬è©¦ ({iterations} è¿­ä»£):")
                    
                    # å•Ÿå‹•ç›£æ§
                    self.start_gpu_monitoring()
                    
                    # æ•ˆèƒ½æ¸¬è©¦
                    model.train()
                    torch.cuda.reset_peak_memory_stats()
                    
                    start_time = time.time()
                    total_loss = 0
                    
                    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦
                    for i in tqdm(range(iterations), desc=f"    Batch {batch_size}", leave=False):
                        if dataloader and i % 10 == 0:
                            # æ¯ 10 æ¬¡è¿­ä»£ä½¿ç”¨ä¸€æ¬¡çœŸå¯¦è³‡æ–™
                            try:
                                real_imgs, real_targets, _, _ = next(iter(dataloader))
                                # èª¿æ•´åˆ°ç›®æ¨™ batch size
                                if real_imgs.size(0) != batch_size:
                                    indices = torch.randint(0, real_imgs.size(0), (batch_size,))
                                    test_input = real_imgs[indices].to(self.device)
                                    test_targets = real_targets[indices].to(self.device) if real_targets is not None else self.create_realistic_targets(batch_size)
                                else:
                                    test_input = real_imgs.to(self.device)
                                    test_targets = real_targets.to(self.device) if real_targets is not None else self.create_realistic_targets(batch_size)
                            except:
                                test_input = torch.randn(batch_size, 3, img_size, img_size).to(self.device)
                                test_targets = self.create_realistic_targets(batch_size)
                        else:
                            # ä½¿ç”¨æ¨¡æ“¬è³‡æ–™
                            test_input = torch.randn(batch_size, 3, img_size, img_size).to(self.device)
                            test_targets = self.create_realistic_targets(batch_size)
                        
                        # å‰å‘+åå‘å‚³æ’­
                        with torch.cuda.amp.autocast():
                            outputs = model(test_input)
                            loss, loss_items = compute_loss(outputs, test_targets)
                            total_loss += loss.item()
                            
                        # åå‘å‚³æ’­
                        loss.backward()
                        model.zero_grad()
                        
                        # æ¸…ç†è®Šæ•¸
                        del test_input, test_targets, outputs, loss
                    
                    end_time = time.time()
                    
                    # åœæ­¢ç›£æ§
                    self.stop_gpu_monitoring()
                    
                    # è¨ˆç®—çµ±è¨ˆ
                    avg_time = (end_time - start_time) / iterations
                    peak_memory = torch.cuda.max_memory_allocated() / 1024**3
                    avg_loss = total_loss / iterations
                    
                    # åˆ†æ GPU çµ±è¨ˆ
                    gpu_analysis = self.analyze_gpu_stats()
                    
                    batch_results[level] = {
                        'iterations': iterations,
                        'total_time': end_time - start_time,
                        'avg_time_per_batch': avg_time,
                        'peak_memory_gb': peak_memory,
                        'avg_loss': avg_loss,
                        'fps': batch_size / avg_time,
                        'gpu_utilization_avg': gpu_analysis['avg_utilization'],
                        'gpu_temperature_max': gpu_analysis['max_temperature'],
                        'successful': True
                    }
                    
                    print(f"    â±ï¸  å¹³å‡æ™‚é–“: {avg_time:.3f}s | ğŸ“Š FPS: {batch_size/avg_time:.1f}")
                    print(f"    ğŸ’¾ å³°å€¼è¨˜æ†¶é«”: {peak_memory:.2f}GB | ğŸŒ¡ï¸  æœ€é«˜æº«åº¦: {gpu_analysis['max_temperature']:.1f}Â°C")
                    print(f"    ğŸ“ˆ GPU ä½¿ç”¨ç‡: {gpu_analysis['avg_utilization']:.1f}% | ğŸ“‰ å¹³å‡ Loss: {avg_loss:.4f}")
                    
                except RuntimeError as e:
                    self.stop_gpu_monitoring()
                    if "out of memory" in str(e):
                        batch_results[level] = {
                            'error': 'Out of Memory',
                            'successful': False
                        }
                        print(f"    âŒ è¨˜æ†¶é«”ä¸è¶³!")
                        torch.cuda.empty_cache()
                        break  # å¦‚æœé€™å€‹ç´šåˆ¥ OOMï¼Œè·³éæ›´é‡çš„æ¸¬è©¦
                    else:
                        batch_results[level] = {
                            'error': str(e),
                            'successful': False
                        }
                        print(f"    âŒ éŒ¯èª¤: {e}")
                except KeyboardInterrupt:
                    self.stop_gpu_monitoring()
                    print(f"    â¹ï¸  ä½¿ç”¨è€…ä¸­æ–·æ¸¬è©¦")
                    batch_results[level] = {
                        'error': 'User Interrupted',
                        'successful': False
                    }
                    break
            
            results[batch_size] = batch_results
        
        return results
    
    def analyze_gpu_stats(self):
        """åˆ†æ GPU ç›£æ§çµ±è¨ˆ"""
        if not self.gpu_stats:
            return {'avg_utilization': 0, 'max_temperature': 0, 'avg_memory_usage': 0}
        
        utilizations = [stat['utilization'] for stat in self.gpu_stats]
        temperatures = [stat['temperature'] for stat in self.gpu_stats]
        memory_usages = [stat['memory_used'] / stat['memory_total'] * 100 for stat in self.gpu_stats]
        
        return {
            'avg_utilization': sum(utilizations) / len(utilizations),
            'max_temperature': max(temperatures) if temperatures else 0,
            'avg_memory_usage': sum(memory_usages) / len(memory_usages)
        }
    
    def create_realistic_targets(self, batch_size):
        """å»ºç«‹æ›´æ“¬çœŸçš„æ¸¬è©¦æ¨™ç±¤"""
        targets = []
        for i in range(batch_size):
            # æ¯å¼µåœ–ç‰‡éš¨æ©Ÿ 1-5 å€‹ç›®æ¨™ç‰©ä»¶ (æ›´æ¥è¿‘ COCO åˆ†å¸ƒ)
            num_objects = torch.randint(1, 6, (1,)).item()
            for _ in range(num_objects):
                targets.append([
                    i,  # batch_idx
                    torch.randint(0, 80, (1,)).item(),  # class (COCO 80 é¡)
                    torch.rand(1).item(),  # x center [0,1]
                    torch.rand(1).item(),  # y center [0,1] 
                    torch.rand(1).item() * 0.5 + 0.1,  # width [0.1,0.6]
                    torch.rand(1).item() * 0.5 + 0.1   # height [0.1,0.6]
                ])
        return torch.tensor(targets, dtype=torch.float32).to(self.device)
    
    def generate_extended_batch_sizes(self, gpu_type, max_batch_size):
        """æ ¹æ“š GPU é¡å‹å’Œæœ€å¤§ batch size ç”Ÿæˆæ“´å±•æ¸¬è©¦ç¯„åœ"""
        base_sizes = self.config['gpu_configs'][gpu_type]['optimal_batch_sizes']
        
        # æ“´å±•ç¯„åœï¼šå¾åŸºç¤ç¯„åœåˆ°æ‰¾åˆ°çš„æœ€å¤§å€¼
        extended_sizes = set(base_sizes)
        
        # æ·»åŠ æ›´å¤šæ¸¬è©¦é»
        current = max(base_sizes)
        while current < max_batch_size:
            current = int(current * 1.5)  # æ¯æ¬¡å¢åŠ  50%
            if current <= max_batch_size:
                extended_sizes.add(current)
        
        # ç¢ºä¿åŒ…å«æœ€å¤§å€¼
        extended_sizes.add(max_batch_size)
        
        return sorted(list(extended_sizes))
    
    def run_comprehensive_benchmark(self, gpu_type=None, test_levels=['light', 'medium', 'heavy'], find_limit=True):
        """åŸ·è¡Œå®Œæ•´æ“¬çœŸæ•ˆèƒ½æ¸¬è©¦"""
        if gpu_type is None:
            gpu_type = self.detect_gpu()
        
        if gpu_type not in self.config['gpu_configs']:
            print(f"æœªçŸ¥çš„ GPU é¡å‹: {gpu_type}")
            gpu_type = "RTX4090"  # é è¨­å€¼
            
        gpu_config = self.config['gpu_configs'][gpu_type]
        print(f"ğŸš€ ä½¿ç”¨ {gpu_config['name']} è¨­å®šé€²è¡Œæ“¬çœŸæ¸¬è©¦")
        print(f"ğŸ“‹ æ¸¬è©¦ç´šåˆ¥: {', '.join(test_levels)}")
        
        # å»ºç«‹æ¨¡å‹å’Œ loss è¨ˆç®—å™¨
        print("ğŸ—ï¸  æº–å‚™æ¨¡å‹...")
        model = self.create_model()
        compute_loss = ComputeLoss(model)
        
        # æ­¥é©Ÿ 1: å°‹æ‰¾æœ€å¤§ batch size (å¦‚æœå•Ÿç”¨)
        max_batch_size = None
        if find_limit:
            start_batch = max(gpu_config['optimal_batch_sizes']) * 2  # å¾å·²çŸ¥æœ€å¤§çš„ 2 å€é–‹å§‹
            max_batch_size = self.find_max_batch_size(model, None, compute_loss, start_batch)
        
        # æ­¥é©Ÿ 2: ç”Ÿæˆæ“´å±•çš„ batch size ç¯„åœ
        if max_batch_size:
            batch_sizes = self.generate_extended_batch_sizes(gpu_type, max_batch_size)
        else:
            batch_sizes = gpu_config['optimal_batch_sizes']
        
        print(f"ğŸ“Š æ¸¬è©¦ batch sizes: {batch_sizes}")
        
        # ä¼°ç®—ç¸½æ™‚é–“
        estimated_time = self.estimate_test_time(batch_sizes, test_levels, gpu_type)
        print(f"â° é ä¼°æ¸¬è©¦æ™‚é–“: {estimated_time:.1f} åˆ†é˜")
        
        # æ­¥é©Ÿ 3: åŸ·è¡Œæ“¬çœŸæ¸¬è©¦
        results = self.benchmark_batch_size_realistic(model, batch_sizes, test_levels=test_levels)
        
        # å„²å­˜çµæœ
        self.results[gpu_type] = {
            'gpu_info': gpu_config,
            'max_batch_size_found': max_batch_size,
            'benchmark_results': results,
            'test_levels': test_levels,
            'system_info': {
                'cpu_count': psutil.cpu_count(),
                'memory_gb': psutil.virtual_memory().total / 1024**3,
                'pytorch_version': torch.__version__,
                'cuda_version': torch.version.cuda
            }
        }
        
        return results
    
    def estimate_test_time(self, batch_sizes, test_levels, gpu_type):
        """ä¼°ç®—æ¸¬è©¦æ™‚é–“ï¼ˆåˆ†é˜ï¼‰"""
        # åŸºæ–¼ RTX 4090 åŸºæº–æ™‚é–“ä¼°ç®—
        base_time_per_iteration = 0.123  # ç§’ (ä¾†è‡ªæ‚¨çš„æ¸¬è©¦çµæœ)
        
        # GPU é€Ÿåº¦å€æ•¸
        speed_multipliers = {
            'RTX4090': 1.0,
            'RTX5090': 0.75,  # å¿« 25%
            'H100': 0.4,      # å¿« 2.5å€
            'B200': 0.25      # å¿« 4å€
        }
        
        multiplier = speed_multipliers.get(gpu_type, 1.0)
        
        total_iterations = 0
        for batch_size in batch_sizes:
            for level in test_levels:
                iterations = {'light': 20, 'medium': 100, 'heavy': 200}[level]
                total_iterations += iterations
        
        # é¡å¤–åŠ ä¸Š OOM æœå°‹æ™‚é–“
        oom_search_time = 2.0  # åˆ†é˜
        
        estimated_seconds = total_iterations * base_time_per_iteration * multiplier
        estimated_minutes = estimated_seconds / 60 + oom_search_time
        
        return estimated_minutes
    
    def save_results(self, output_file="benchmark_results.yaml"):
        """å„²å­˜æ¸¬è©¦çµæœ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            yaml.dump(self.results, f, default_flow_style=False, allow_unicode=True)
        print(f"çµæœå·²å„²å­˜è‡³: {output_file}")
    
    def print_comprehensive_summary(self, gpu_type):
        """åˆ—å°æ“¬çœŸæ¸¬è©¦æ‘˜è¦"""
        if gpu_type not in self.results:
            return
            
        data = self.results[gpu_type]
        results = data['benchmark_results']
        
        print(f"\n{'='*80}")
        print(f"ğŸ† {gpu_type} æ“¬çœŸæ•ˆèƒ½æ¸¬è©¦å ±å‘Š")
        print(f"{'='*80}")
        
        # GPU åŸºæœ¬è³‡è¨Š
        gpu_info = data['gpu_info']
        sys_info = data['system_info']
        print(f"ğŸ–¥ï¸  GPU: {gpu_info['name']} ({gpu_info['memory_gb']}GB)")
        print(f"ğŸ’» ç³»çµ±: {sys_info['cpu_count']} æ ¸å¿ƒ, {sys_info['memory_gb']:.1f}GB RAM")
        print(f"ğŸ ç’°å¢ƒ: PyTorch {sys_info['pytorch_version']}, CUDA {sys_info['cuda_version']}")
        
        if data.get('max_batch_size_found'):
            print(f"ğŸ¯ æœ€å¤§ Batch Size: {data['max_batch_size_found']}")
        
        print(f"\n{'Batch':<8} {'Level':<8} {'Status':<8} {'Time/Batch':<12} {'Memory':<10} {'FPS':<8} {'GPU%':<6} {'Temp':<6}")
        print("-" * 80)
        
        # è©³ç´°çµæœè¡¨æ ¼
        best_fps = 0
        best_config = None
        
        for batch_size, batch_results in results.items():
            for level, result in batch_results.items():
                if result['successful']:
                    fps = result['fps']
                    if fps > best_fps:
                        best_fps = fps
                        best_config = (batch_size, level)
                    
                    print(f"{batch_size:<8} {level:<8} {'âœ…':<8} {result['avg_time_per_batch']:.3f}s{'':<6} "
                          f"{result['peak_memory_gb']:.1f}GB{'':<4} {fps:<8.0f} "
                          f"{result['gpu_utilization_avg']:.0f}%{'':<3} {result['gpu_temperature_max']:.0f}Â°C")
                else:
                    error_msg = result.get('error', 'æœªçŸ¥éŒ¯èª¤')[:10]
                    print(f"{batch_size:<8} {level:<8} {'âŒ':<8} {error_msg}")
        
        # æ•ˆèƒ½æ‘˜è¦
        print(f"\nğŸ† æœ€ä½³æ•ˆèƒ½é…ç½®:")
        if best_config:
            batch_size, level = best_config
            best_result = results[batch_size][level]
            print(f"   Batch Size: {batch_size} ({level} ç´šåˆ¥)")
            print(f"   æœ€é«˜ FPS: {best_fps:.0f}")
            print(f"   è¨˜æ†¶é«”ä½¿ç”¨: {best_result['peak_memory_gb']:.1f}GB")
            print(f"   GPU ä½¿ç”¨ç‡: {best_result['gpu_utilization_avg']:.0f}%")
    
    def generate_cross_gpu_comparison(self, gpu_types):
        """ç”Ÿæˆè·¨ GPU æ•ˆèƒ½æ¯”è¼ƒå ±å‘Š"""
        print(f"\n{'='*100}")
        print(f"ğŸ”¥ è·¨ GPU æ•ˆèƒ½æ¯”è¼ƒå ±å‘Š")
        print(f"{'='*100}")
        
        comparison_data = []
        for gpu_type in gpu_types:
            if gpu_type in self.results:
                data = self.results[gpu_type]
                
                # æ‰¾å‡ºæœ€ä½³æ•ˆèƒ½é…ç½®
                best_fps = 0
                best_batch = None
                max_batch = data.get('max_batch_size_found', 'N/A')
                
                for batch_size, batch_results in data['benchmark_results'].items():
                    for level, result in batch_results.items():
                        if result['successful'] and result['fps'] > best_fps:
                            best_fps = result['fps']
                            best_batch = batch_size
                
                comparison_data.append({
                    'gpu': gpu_type,
                    'memory_gb': data['gpu_info']['memory_gb'],
                    'best_fps': best_fps,
                    'best_batch': best_batch,
                    'max_batch': max_batch
                })
        
        # æ’åºä¸¦é¡¯ç¤º
        comparison_data.sort(key=lambda x: x['best_fps'], reverse=True)
        
        print(f"{'GPU':<10} {'è¨˜æ†¶é«”':<8} {'æœ€ä½³FPS':<10} {'æœ€ä½³Batch':<10} {'æ¥µé™Batch':<10} {'ç›¸å°æ•ˆèƒ½':<8}")
        print("-" * 70)
        
        baseline_fps = comparison_data[-1]['best_fps'] if comparison_data else 1
        
        for data in comparison_data:
            relative_perf = data['best_fps'] / baseline_fps
            print(f"{data['gpu']:<10} {data['memory_gb']}GB{'':<4} {data['best_fps']:<10.0f} "
                  f"{data['best_batch']:<10} {data['max_batch']:<10} {relative_perf:.1f}x")
        
        return comparison_data

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GPU æ“¬çœŸæ•ˆèƒ½æ¸¬è©¦ v2.0")
    parser.add_argument("--gpu-type", type=str, help="æŒ‡å®š GPU é¡å‹ (RTX4090, RTX5090, H100, B200)")
    parser.add_argument("--config", type=str, default="configs/gpu_configs.yaml", help="é…ç½®æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--output", type=str, default="benchmark_results_v2.yaml", help="è¼¸å‡ºæª”æ¡ˆ")
    parser.add_argument("--test-levels", nargs='+', default=['light', 'medium', 'heavy'], 
                        help="æ¸¬è©¦ç´šåˆ¥ (light, medium, heavy)")
    parser.add_argument("--find-limit", action='store_true', default=True, help="å°‹æ‰¾æœ€å¤§ batch size")
    parser.add_argument("--quick", action='store_true', help="å¿«é€Ÿæ¸¬è©¦ (åƒ… light ç´šåˆ¥)")
    parser.add_argument("--compare", nargs='+', help="æ¯”è¼ƒå¤šå€‹ GPU é¡å‹ (éœ€è¦å…ˆåŸ·è¡Œå„åˆ¥æ¸¬è©¦)")
    
    args = parser.parse_args()
    
    # å¿«é€Ÿæ¸¬è©¦æ¨¡å¼
    if args.quick:
        args.test_levels = ['light']
        args.find_limit = False
    
    benchmark = GPUBenchmark(args.config)
    
    # æ¯”è¼ƒæ¨¡å¼
    if args.compare:
        print("ğŸ”„ è¼‰å…¥ä¹‹å‰çš„æ¸¬è©¦çµæœé€²è¡Œæ¯”è¼ƒ...")
        try:
            with open(args.output, 'r', encoding='utf-8') as f:
                benchmark.results = yaml.safe_load(f)
            benchmark.generate_cross_gpu_comparison(args.compare)
        except FileNotFoundError:
            print("âŒ æ‰¾ä¸åˆ°ä¹‹å‰çš„æ¸¬è©¦çµæœï¼Œè«‹å…ˆåŸ·è¡Œå„ GPU çš„æ¸¬è©¦")
    else:
        # å–®ä¸€ GPU æ¸¬è©¦
        detected_gpu = benchmark.detect_gpu()
        target_gpu = args.gpu_type or detected_gpu
        
        print(f"ğŸ® é–‹å§‹ {target_gpu} æ“¬çœŸæ•ˆèƒ½æ¸¬è©¦...")
        results = benchmark.run_comprehensive_benchmark(
            target_gpu, 
            args.test_levels, 
            args.find_limit
        )
        
        benchmark.print_comprehensive_summary(target_gpu)
        benchmark.save_results(args.output)
        
        print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼çµæœå·²å„²å­˜è‡³ {args.output}")
        print(f"ğŸ’¡ ä½¿ç”¨ --compare åƒæ•¸å¯æ¯”è¼ƒå¤šå€‹ GPU çµæœ")