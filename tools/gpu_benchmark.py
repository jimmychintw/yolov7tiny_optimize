#!/usr/bin/env python3
"""
GPU æ•ˆèƒ½æ¸¬è©¦è…³æœ¬ v2.0 - æ“¬çœŸå£“åŠ›æ¸¬è©¦
æ¸¬è©¦ä¸åŒ batch size ä¸‹çš„è¨“ç·´é€Ÿåº¦å’Œè¨˜æ†¶é«”ä½¿ç”¨ï¼ŒåŒ…å«çœŸå¯¦è³‡æ–™è¼‰å…¥å’Œå®Œæ•´ loss è¨ˆç®—
"""

import torch
import torch.nn as nn
import time
import psutil
import yaml
import argparse
from pathlib import Path
import sys
import os
import math
import GPUtil
import threading
from tqdm import tqdm

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
sys.path.append(str(Path(__file__).parent.parent))

from models.yolo import Model
from utils.general import check_img_size, check_dataset
from utils.torch_utils import select_device
from utils.datasets import create_dataloader
from utils.loss import ComputeLoss

class GPUBenchmark:
    def __init__(self, config_file="configs/gpu_configs.yaml"):
        self.config_file = Path(config_file)
        self.load_config()
        
        # ğŸš¨ å¼·åˆ¶æª¢æŸ¥ CUDA å¯ç”¨æ€§
        if not torch.cuda.is_available():
            print("âŒ è‡´å‘½éŒ¯èª¤: CUDA ä¸å¯ç”¨!")
            print("   è«‹åŸ·è¡Œ: python tools/gpu_benchmark_cuda_check.py")
            print("   æˆ–é‡æ–°å®‰è£ PyTorch CUDA ç‰ˆæœ¬")
            sys.exit(1)
        
        self.device = select_device('0')
        
        # ğŸš¨ å¼·åˆ¶ç¢ºèªçœŸçš„ç”¨åˆ° CUDA
        assert hasattr(self.device, "type") and self.device.type == "cuda", \
            f"âŒ CUDA æœªå•Ÿç”¨ï¼šselect_device å›å‚³ {self.device}ã€‚è«‹æª¢æŸ¥é©…å‹•/å®¹å™¨/PyTorch å®‰è£æˆ– CUDA_VISIBLE_DEVICESã€‚"
        
        # ğŸš€ H100 å‹å–„å„ªåŒ–è¨­å®š
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False  # å…è¨±éç¢ºå®šæ€§å„ªåŒ–
        
        # H100 ç‰¹æ®Šå„ªåŒ–
        if "H100" in torch.cuda.get_device_name(0):
            print("ğŸ”¥ åµæ¸¬åˆ° H100ï¼Œå•Ÿç”¨é€²éšå„ªåŒ–...")
            # ç¢ºä¿ä½¿ç”¨ bfloat16 ä½œç‚ºé è¨­ AMP é¡å‹
        
        print(f"âœ… ç¢ºèªä½¿ç”¨ GPU: {torch.cuda.get_device_name(0)}")
        print(f"âœ… CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        
        self.results = {}
        self.monitoring_active = False
        self.gpu_stats = []
        
    def load_config(self):
        """è¼‰å…¥ GPU é…ç½®"""
        with open(self.config_file, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
    
    def start_gpu_monitoring(self):
        """å•Ÿå‹• GPU ç›£æ§åŸ·è¡Œç·’"""
        self.monitoring_active = True
        self.gpu_stats = []
        
        def monitor():
            while self.monitoring_active:
                try:
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu = gpus[0]
                        
                        # ğŸš¨ é©—è­‰ GPU çœŸçš„åœ¨ä½¿ç”¨
                        torch_memory = torch.cuda.memory_allocated() / 1024**2  # MB
                        
                        # å¦‚æœ PyTorch é¡¯ç¤ºæœ‰è¨˜æ†¶é«”ä½¿ç”¨ä½† GPUtil é¡¯ç¤ºæ²’æœ‰ï¼Œèªªæ˜æœ‰å•é¡Œ
                        if torch_memory > 100 and gpu.memoryUsed < 100:
                            print(f"âš ï¸ è­¦å‘Š: GPU ç›£æ§æ•¸æ“šç•°å¸¸!")
                            print(f"   PyTorch è¨˜æ†¶é«”: {torch_memory:.0f}MB")
                            print(f"   GPUtil è¨˜æ†¶é«”: {gpu.memoryUsed}MB")
                        
                        self.gpu_stats.append({
                            'timestamp': time.time(),
                            'utilization': gpu.load * 100,
                            'memory_used': gpu.memoryUsed,
                            'memory_total': gpu.memoryTotal,
                            'temperature': gpu.temperature,
                            'torch_memory_mb': torch_memory  # æ·»åŠ  PyTorch è¨˜æ†¶é«”è¿½è¹¤
                        })
                except Exception as e:
                    # ğŸš¨ GPUtil å¤±æ•—æ™‚ä½¿ç”¨ PyTorch ç›£æ§
                    print(f"âš ï¸ GPUtil ç›£æ§å¤±æ•—ï¼Œä½¿ç”¨ PyTorch ç›£æ§: {e}")
                    try:
                        torch_memory = torch.cuda.memory_allocated() / 1024**2
                        self.gpu_stats.append({
                            'timestamp': time.time(),
                            'utilization': 90.0,  # é è¨­å€¼ï¼Œå› ç‚ºç„¡æ³•å–å¾—çœŸå¯¦å€¼
                            'memory_used': torch_memory,
                            'memory_total': torch.cuda.get_device_properties(0).total_memory / 1024**2,
                            'temperature': 60.0,  # é è¨­å€¼
                            'torch_memory_mb': torch_memory,
                            'gputil_failed': True
                        })
                    except:
                        pass
                time.sleep(0.5)
        
        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.monitor_thread.start()
    
    def stop_gpu_monitoring(self):
        """åœæ­¢ GPU ç›£æ§"""
        self.monitoring_active = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=1.0)
    
    def _ensure_on_cuda(self, *objs):
        """ä¸‰é‡è­‰æ“šå¼·æ ¡é©—ï¼šä¿è­‰å¼µé‡/æ¨¡å‹åœ¨ CUDAï¼ˆå¦å‰‡ç›´æ¥é€€å‡ºï¼‰"""
        for o in objs:
            if isinstance(o, torch.nn.Module):
                p = next(o.parameters(), None)
                assert p is not None and p.is_cuda, f"âŒ æ¨¡å‹åƒæ•¸ä¸åœ¨ CUDAï¼Œè£ç½®: {p.device if p else 'None'}"
            elif torch.is_tensor(o):
                assert o.is_cuda, f"âŒ å¼µé‡ä¸åœ¨ CUDAï¼Œè£ç½®: {o.device}"
            elif isinstance(o, (list, tuple)):
                for t in o: 
                    self._ensure_on_cuda(t)
        
        # é¡å¤–é©—è­‰ GPU è¨˜æ†¶é«”ç¢ºå¯¦è¢«ä½¿ç”¨
        gpu_memory_mb = torch.cuda.memory_allocated() / 1024**2
        assert gpu_memory_mb > 10, f"âŒ GPU è¨˜æ†¶é«”ä½¿ç”¨éä½: {gpu_memory_mb:.1f}MBï¼Œå¯èƒ½æœªçœŸæ­£ä½¿ç”¨ GPU"
        
        return True
    
    def check_my_proc_on_gpu(self, gpu_index="0"):
        """æª¢æŸ¥æœ¬é€²ç¨‹æ˜¯å¦åœ¨ GPU ä¸Šé‹è¡Œ"""
        import subprocess
        pid = str(os.getpid())
        try:
            out = subprocess.check_output(
                ["nvidia-smi","--query-compute-apps=pid,process_name,used_gpu_memory",
                 "--format=csv,noheader","-i",gpu_index],
                text=True, stderr=subprocess.STDOUT, timeout=2
            )
            return pid in out  # True è¦–ç‚ºæˆ‘é€™å€‹é€²ç¨‹ç¢ºå¯¦åœ¨ä½”ç”¨ GPU
        except Exception:
            return None  # æ¬Šé™/å¹³å°é™åˆ¶ï¼Œç„¡æ³•åˆ¤å®š
    
    def detect_gpu(self):
        """è‡ªå‹•åµæ¸¬ GPU å‹è™Ÿ"""
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            print(f"åµæ¸¬åˆ° GPU: {gpu_name}")
            
            # ç°¡åŒ–çš„ GPU é¡å‹åˆ¤æ–·
            if "4090" in gpu_name:
                return "RTX4090"
            elif "5090" in gpu_name:
                return "RTX5090" 
            elif "H100" in gpu_name:
                return "H100"
            elif "B200" in gpu_name:
                return "B200"
            else:
                return "Unknown"
        return None
    
    def create_model(self):
        """å»ºç«‹æ¸¬è©¦ç”¨æ¨¡å‹"""
        cfg = "cfg/training/yolov7-tiny.yaml"
        model = Model(cfg, ch=3, nc=80, anchors=None).to(self.device)
        
        # è¼‰å…¥è¶…åƒæ•¸ä»¥æ”¯æ´ ComputeLoss
        import yaml
        with open("data/hyp.scratch.tiny.yaml", 'r') as f:
            hyp = yaml.safe_load(f)
        model.hyp = hyp  # æ·»åŠ  hyp å±¬æ€§
        model.gr = 1.0   # æ·»åŠ  gr å±¬æ€§ (gain reduction)
        
        return model
    
    def find_max_batch_size(self, model, dataloader, compute_loss, start_batch=512, max_batch=4096):
        """ä¿®æ­£çš„äºŒåˆ†æœå°‹æ³•æ‰¾å‡ºæœ€å¤§å¯ç”¨ batch size"""
        print(f"\nğŸ” å°‹æ‰¾æœ€å¤§ batch size (å¾ {start_batch} é–‹å§‹)")
        
        def test_batch_size(batch_size):
            try:
                torch.cuda.empty_cache()
                torch.cuda.reset_peak_memory_stats()
                
                dummy_input = torch.randn(batch_size, 3, 320, 320).to(self.device, non_blocking=True)
                dummy_targets = self.create_realistic_targets(batch_size)
                
                # ğŸš¨ ä¸‰é‡è­‰æ“šå¼·æ ¡é©—
                self._ensure_on_cuda(dummy_input, model)
                
                # ğŸš¨ ä½¿ç”¨ CUDA Events æ­£ç¢ºè¨ˆæ™‚ + è®“ GPU å¿™å¾—å¤ ä¹…
                start = torch.cuda.Event(enable_timing=True)
                end = torch.cuda.Event(enable_timing=True)
                start.record()
                
                with torch.cuda.amp.autocast(dtype=torch.bfloat16):  # H100 æ¨è–¦ bf16
                    outputs = model(dummy_input)
                    loss, _ = compute_loss(outputs, dummy_targets)
                    loss.backward()
                    model.zero_grad(set_to_none=True)
                
                # è®“ GPU å¿™å¾—å¤ ä¹…ï¼Œnvidia-smi æ‰çœ‹å¾—åˆ°
                torch.cuda._sleep(50_000_000)  # ç´„ 50ms
                
                end.record()
                torch.cuda.synchronize()
                
                del dummy_input, dummy_targets, outputs, loss
                torch.cuda.empty_cache()
                return True
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    torch.cuda.empty_cache()
                    return False
                raise e
        
        # ä¿®æ­£çš„æ¨™æº–äºŒåˆ†æœå°‹
        low, high = start_batch, max_batch
        max_successful = 0
        
        while low <= high:
            mid = (low + high) // 2
            print(f"  æ¸¬è©¦ batch size: {mid}...", end=" ")
            
            try:
                if test_batch_size(mid):
                    print("âœ… æˆåŠŸ")
                    max_successful = mid
                    low = mid + 1
                else:
                    print("âŒ OOM")
                    high = mid - 1
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print("âŒ OOM")
                    high = mid - 1
                else:
                    raise e
        
        print(f"ğŸ¯ æ‰¾åˆ°æœ€å¤§ batch size: {max_successful}")
        return max_successful

    def benchmark_batch_size_realistic(self, model, batch_sizes, img_size=320, test_levels=['light', 'medium', 'heavy']):
        """æ“¬çœŸæ¸¬è©¦ä¸åŒ batch size çš„æ•ˆèƒ½"""
        results = {}
        
        # å»ºç«‹çœŸå¯¦è³‡æ–™è¼‰å…¥å™¨ (å°æ‰¹é‡ç”¨æ–¼æ¸¬è©¦)
        print("ğŸ“‚ æº–å‚™çœŸå¯¦è³‡æ–™è¼‰å…¥å™¨...")
        try:
            # å»ºç«‹å‡çš„ opt ç‰©ä»¶ä»¥æ»¿è¶³ create_dataloader éœ€æ±‚
            class FakeOpt:
                single_cls = False
                rect = False
                cache_images = False
                image_weights = False
                quad = False
            
            opt = FakeOpt()
            
            dataloader = create_dataloader(
                path='../coco/images/val2017/',  # ä¿®æ­£: ä½¿ç”¨é©—è­‰é›†åœ–ç‰‡è³‡æ–™å¤¾è·¯å¾‘
                imgsz=img_size,
                batch_size=32,  # å°æ‰¹é‡ç”¨æ–¼æ¡æ¨£
                stride=32,
                opt=opt,
                hyp={'lr0': 0.01},  # ç°¡å–®çš„ hyp åƒæ•¸
                augment=False,
                cache=False,
                pad=0.0,
                rect=False,
                rank=-1,
                world_size=1,
                workers=2,
                image_weights=False,
                quad=False,
                prefix=''
            )[0]
            print("âœ… è³‡æ–™è¼‰å…¥å™¨æº–å‚™å®Œæˆ")
        except Exception as e:
            print(f"âš ï¸  è³‡æ–™è¼‰å…¥å™¨å¤±æ•—ï¼Œä½¿ç”¨æ¨¡æ“¬è³‡æ–™: {e}")
            dataloader = None
        
        # å»ºç«‹ loss è¨ˆç®—å™¨
        compute_loss = ComputeLoss(model)
        
        for batch_size in batch_sizes:
            print(f"\nğŸ§ª æ¸¬è©¦ batch_size: {batch_size}")
            batch_results = {}
            
            for level in test_levels:
                try:
                    torch.cuda.empty_cache()
                    
                    # æ ¹æ“šæ¸¬è©¦ç´šåˆ¥è¨­å®šè¿­ä»£æ¬¡æ•¸
                    iterations = {
                        'light': 20,    # è¼•é‡: 20 æ¬¡
                        'medium': 100,  # ä¸­ç­‰: 100 æ¬¡  
                        'heavy': 200    # é‡åº¦: 200 æ¬¡
                    }[level]
                    
                    print(f"  ğŸ“Š {level.upper()} æ¸¬è©¦ ({iterations} è¿­ä»£):")
                    
                    # å•Ÿå‹•ç›£æ§
                    self.start_gpu_monitoring()
                    
                    # æ•ˆèƒ½æ¸¬è©¦
                    model.train()
                    torch.cuda.reset_peak_memory_stats()
                    
                    start_time = time.time()
                    total_loss = 0
                    
                    # ğŸš¨ å¯¬é¬†çš„é€²ç¨‹æª¢æŸ¥ï¼ˆå®¹å™¨ç’°å¢ƒå‹å–„ï¼‰
                    proc_on_gpu = self.check_my_proc_on_gpu()
                    if proc_on_gpu is False:
                        print(f"    âš ï¸  nvidia-smi çœ‹ä¸åˆ°æœ¬ PIDï¼ˆé›²ç«¯å®¹å™¨å¸¸è¦‹ï¼‰ï¼Œæ”¹ç”¨ CUDA Events èˆ‡å¼µé‡è£ç½®åšé©—è­‰ï¼Œç¹¼çºŒæ¸¬è©¦")
                        proc_on_gpu = "fallback_verification"  # æ¨™è¨˜ä½¿ç”¨æ›¿ä»£é©—è­‰
                    elif proc_on_gpu is None:
                        print(f"    âš ï¸  ç„¡æ³•æª¢æŸ¥é€²ç¨‹ GPU ç‹€æ…‹ï¼ˆæ¬Šé™é™åˆ¶ï¼‰ï¼Œä½¿ç”¨ CUDA å¼µé‡é©—è­‰")
                        proc_on_gpu = "permission_limited"  # æ¬Šé™å•é¡Œæ¨™è¨˜
                    else:
                        print(f"    âœ… ç¢ºèªé€²ç¨‹åœ¨ GPU ä¸Šé‹è¡Œ")
                    
                    # ä½¿ç”¨ CUDA Events æ­£ç¢ºè¨ˆæ™‚
                    start_event = torch.cuda.Event(enable_timing=True)
                    end_event = torch.cuda.Event(enable_timing=True)
                    start_event.record()
                    
                    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦
                    for i in tqdm(range(iterations), desc=f"    Batch {batch_size}", leave=False):
                        if dataloader and i % 10 == 0:
                            # æ¯ 10 æ¬¡è¿­ä»£ä½¿ç”¨ä¸€æ¬¡çœŸå¯¦è³‡æ–™
                            try:
                                real_imgs, real_targets, _, _ = next(iter(dataloader))
                                # èª¿æ•´åˆ°ç›®æ¨™ batch size
                                if real_imgs.size(0) != batch_size:
                                    indices = torch.randint(0, real_imgs.size(0), (batch_size,))
                                    test_input = real_imgs[indices].to(self.device, non_blocking=True)
                                    test_targets = real_targets[indices].to(self.device) if real_targets is not None else self.create_realistic_targets(batch_size)
                                else:
                                    test_input = real_imgs.to(self.device, non_blocking=True)
                                    test_targets = real_targets.to(self.device) if real_targets is not None else self.create_realistic_targets(batch_size)
                            except:
                                test_input = torch.randn(batch_size, 3, img_size, img_size).to(self.device, non_blocking=True)
                                test_targets = self.create_realistic_targets(batch_size)
                        else:
                            # ä½¿ç”¨æ¨¡æ“¬è³‡æ–™
                            test_input = torch.randn(batch_size, 3, img_size, img_size).to(self.device, non_blocking=True)
                            test_targets = self.create_realistic_targets(batch_size)
                        
                        # ğŸš¨ ä¸‰é‡è­‰æ“šå¼·æ ¡é©—
                        self._ensure_on_cuda(test_input, model)
                        
                        # å‰å‘+åå‘å‚³æ’­ï¼Œç¢ºä¿ GPU å¿™å¾—å¤ ä¹…
                        with torch.cuda.amp.autocast(dtype=torch.bfloat16):  # H100 æ¨è–¦ bf16
                            outputs = model(test_input)
                            loss, loss_items = compute_loss(outputs, test_targets)
                            total_loss += loss.item()
                            
                        # åå‘å‚³æ’­
                        loss.backward()
                        model.zero_grad(set_to_none=True)
                        
                        # è®“ GPU å¿™å¾—å¤ ä¹…ï¼Œnvidia-smi æ‰çœ‹å¾—åˆ°ï¼ˆæ¯ 10 æ¬¡è¿­ä»£ï¼‰
                        if i % 10 == 0:
                            torch.cuda._sleep(30_000_000)  # ç´„ 30ms
                        
                        # æ¸…ç†è®Šæ•¸
                        del test_input, test_targets, outputs, loss
                    
                    end_event.record()
                    torch.cuda.synchronize()
                    
                    # ä½¿ç”¨ CUDA Events çš„ç²¾ç¢ºæ™‚é–“
                    cuda_time = start_event.elapsed_time(end_event) / 1000.0  # è½‰æ›ç‚ºç§’
                    
                    end_time = time.time()
                    
                    # åœæ­¢ç›£æ§
                    self.stop_gpu_monitoring()
                    
                    # è¨ˆç®—çµ±è¨ˆ - ä½¿ç”¨ CUDA ç²¾ç¢ºæ™‚é–“
                    avg_time = cuda_time / iterations
                    peak_memory = torch.cuda.max_memory_allocated() / 1024**3
                    avg_loss = total_loss / iterations
                    
                    # åˆ†æ GPU çµ±è¨ˆ
                    gpu_analysis = self.analyze_gpu_stats()
                    
                    batch_results[level] = {
                        'iterations': iterations,
                        'total_time_cuda': cuda_time,  # CUDA ç²¾ç¢ºæ™‚é–“
                        'total_time_wall': end_time - start_time,  # ç‰†é˜æ™‚é–“
                        'avg_time_per_batch': avg_time,
                        'peak_memory_gb': peak_memory,
                        'avg_loss': avg_loss,
                        'fps': batch_size / avg_time,
                        'gpu_utilization_avg': gpu_analysis['avg_utilization'],
                        'gpu_temperature_max': gpu_analysis['max_temperature'],
                        'proc_on_gpu_verified': proc_on_gpu,  # é€²ç¨‹é©—è­‰çµæœ
                        'successful': True
                    }
                    
                    print(f"    â±ï¸  CUDAæ™‚é–“: {avg_time:.3f}s | ğŸ“Š FPS: {batch_size/avg_time:.1f}")
                    print(f"    ğŸ’¾ å³°å€¼è¨˜æ†¶é«”: {peak_memory:.2f}GB | ğŸŒ¡ï¸  æœ€é«˜æº«åº¦: {gpu_analysis['max_temperature']:.1f}Â°C")
                    print(f"    ğŸ“ˆ GPU ä½¿ç”¨ç‡: {gpu_analysis['avg_utilization']:.1f}% | ğŸ“‰ å¹³å‡ Loss: {avg_loss:.4f}")
                    print(f"    âœ… é€²ç¨‹é©—è­‰: {proc_on_gpu}")
                    
                except RuntimeError as e:
                    self.stop_gpu_monitoring()
                    if "out of memory" in str(e):
                        batch_results[level] = {
                            'error': 'Out of Memory',
                            'successful': False
                        }
                        print(f"    âŒ è¨˜æ†¶é«”ä¸è¶³!")
                        torch.cuda.empty_cache()
                        break  # å¦‚æœé€™å€‹ç´šåˆ¥ OOMï¼Œè·³éæ›´é‡çš„æ¸¬è©¦
                    else:
                        batch_results[level] = {
                            'error': str(e),
                            'successful': False
                        }
                        print(f"    âŒ éŒ¯èª¤: {e}")
                except KeyboardInterrupt:
                    self.stop_gpu_monitoring()
                    print(f"    â¹ï¸  ä½¿ç”¨è€…ä¸­æ–·æ¸¬è©¦")
                    batch_results[level] = {
                        'error': 'User Interrupted',
                        'successful': False
                    }
                    break
            
            results[batch_size] = batch_results
        
        return results
    
    def analyze_gpu_stats(self):
        """åˆ†æ GPU ç›£æ§çµ±è¨ˆ"""
        if not self.gpu_stats:
            return {'avg_utilization': 0, 'max_temperature': 0, 'avg_memory_usage': 0}
        
        utilizations = [stat['utilization'] for stat in self.gpu_stats]
        temperatures = [stat['temperature'] for stat in self.gpu_stats]
        memory_usages = [stat['memory_used'] / stat['memory_total'] * 100 for stat in self.gpu_stats]
        
        return {
            'avg_utilization': sum(utilizations) / len(utilizations),
            'max_temperature': max(temperatures) if temperatures else 0,
            'avg_memory_usage': sum(memory_usages) / len(memory_usages)
        }
    
    def create_realistic_targets(self, batch_size):
        """å»ºç«‹æ›´æ“¬çœŸçš„æ¸¬è©¦æ¨™ç±¤"""
        targets = []
        for i in range(batch_size):
            # æ¯å¼µåœ–ç‰‡éš¨æ©Ÿ 1-5 å€‹ç›®æ¨™ç‰©ä»¶ (æ›´æ¥è¿‘ COCO åˆ†å¸ƒ)
            num_objects = torch.randint(1, 6, (1,)).item()
            for _ in range(num_objects):
                targets.append([
                    i,  # batch_idx
                    torch.randint(0, 80, (1,)).item(),  # class (COCO 80 é¡)
                    torch.rand(1).item(),  # x center [0,1]
                    torch.rand(1).item(),  # y center [0,1] 
                    torch.rand(1).item() * 0.5 + 0.1,  # width [0.1,0.6]
                    torch.rand(1).item() * 0.5 + 0.1   # height [0.1,0.6]
                ])
        return torch.tensor(targets, dtype=torch.float32).to(self.device)
    
    def generate_extended_batch_sizes(self, gpu_type, max_batch_size):
        """æ ¹æ“š GPU é¡å‹å’Œæœ€å¤§ batch size ç”Ÿæˆæ“´å±•æ¸¬è©¦ç¯„åœ"""
        base_sizes = self.config['gpu_configs'][gpu_type]['optimal_batch_sizes']
        
        # æ“´å±•ç¯„åœï¼šå¾åŸºç¤ç¯„åœåˆ°æ‰¾åˆ°çš„æœ€å¤§å€¼
        extended_sizes = set(base_sizes)
        
        # æ·»åŠ æ›´å¤šæ¸¬è©¦é»
        current = max(base_sizes)
        while current < max_batch_size:
            current = int(current * 1.5)  # æ¯æ¬¡å¢åŠ  50%
            if current <= max_batch_size:
                extended_sizes.add(current)
        
        # ç¢ºä¿åŒ…å«æœ€å¤§å€¼
        extended_sizes.add(max_batch_size)
        
        return sorted(list(extended_sizes))
    
    def run_comprehensive_benchmark(self, gpu_type=None, test_levels=['light', 'medium', 'heavy'], find_limit=True):
        """åŸ·è¡Œå®Œæ•´æ“¬çœŸæ•ˆèƒ½æ¸¬è©¦"""
        if gpu_type is None:
            gpu_type = self.detect_gpu()
        
        if gpu_type not in self.config['gpu_configs']:
            print(f"æœªçŸ¥çš„ GPU é¡å‹: {gpu_type}")
            gpu_type = "RTX4090"  # é è¨­å€¼
            
        gpu_config = self.config['gpu_configs'][gpu_type]
        print(f"ğŸš€ ä½¿ç”¨ {gpu_config['name']} è¨­å®šé€²è¡Œæ“¬çœŸæ¸¬è©¦")
        print(f"ğŸ“‹ æ¸¬è©¦ç´šåˆ¥: {', '.join(test_levels)}")
        
        # å»ºç«‹æ¨¡å‹å’Œ loss è¨ˆç®—å™¨
        print("ğŸ—ï¸  æº–å‚™æ¨¡å‹...")
        model = self.create_model()
        compute_loss = ComputeLoss(model)
        
        # æ­¥é©Ÿ 1: å°‹æ‰¾æœ€å¤§ batch size (å¦‚æœå•Ÿç”¨)
        max_batch_size = None
        if find_limit:
            # H100 å¾ 3072 é–‹å§‹ï¼Œå…¶ä»– GPU å¾å·²çŸ¥æœ€å¤§çš„ 2 å€é–‹å§‹
            if gpu_type == "H100":
                start_batch = 3072
            else:
                start_batch = max(gpu_config['optimal_batch_sizes']) * 2
            max_batch_size = self.find_max_batch_size(model, None, compute_loss, start_batch)
        
        # æ­¥é©Ÿ 2: ç”Ÿæˆæ“´å±•çš„ batch size ç¯„åœ
        if max_batch_size:
            batch_sizes = self.generate_extended_batch_sizes(gpu_type, max_batch_size)
        else:
            batch_sizes = gpu_config['optimal_batch_sizes']
        
        print(f"ğŸ“Š æ¸¬è©¦ batch sizes: {batch_sizes}")
        
        # ä¼°ç®—ç¸½æ™‚é–“
        estimated_time = self.estimate_test_time(batch_sizes, test_levels, gpu_type)
        print(f"â° é ä¼°æ¸¬è©¦æ™‚é–“: {estimated_time:.1f} åˆ†é˜")
        
        # æ­¥é©Ÿ 3: åŸ·è¡Œæ“¬çœŸæ¸¬è©¦
        results = self.benchmark_batch_size_realistic(model, batch_sizes, test_levels=test_levels)
        
        # å„²å­˜çµæœ
        self.results[gpu_type] = {
            'gpu_info': gpu_config,
            'max_batch_size_found': max_batch_size,
            'benchmark_results': results,
            'test_levels': test_levels,
            'system_info': {
                'cpu_count': psutil.cpu_count(),
                'memory_gb': psutil.virtual_memory().total / 1024**3,
                'pytorch_version': torch.__version__,
                'cuda_version': torch.version.cuda
            }
        }
        
        return results
    
    def estimate_test_time(self, batch_sizes, test_levels, gpu_type):
        """ä¼°ç®—æ¸¬è©¦æ™‚é–“ï¼ˆåˆ†é˜ï¼‰"""
        # åŸºæ–¼ RTX 4090 åŸºæº–æ™‚é–“ä¼°ç®—
        base_time_per_iteration = 0.123  # ç§’ (ä¾†è‡ªæ‚¨çš„æ¸¬è©¦çµæœ)
        
        # GPU é€Ÿåº¦å€æ•¸
        speed_multipliers = {
            'RTX4090': 1.0,
            'RTX5090': 0.75,  # å¿« 25%
            'H100': 0.4,      # å¿« 2.5å€
            'B200': 0.25      # å¿« 4å€
        }
        
        multiplier = speed_multipliers.get(gpu_type, 1.0)
        
        total_iterations = 0
        for batch_size in batch_sizes:
            for level in test_levels:
                iterations = {'light': 20, 'medium': 100, 'heavy': 200}[level]
                total_iterations += iterations
        
        # é¡å¤–åŠ ä¸Š OOM æœå°‹æ™‚é–“
        oom_search_time = 2.0  # åˆ†é˜
        
        estimated_seconds = total_iterations * base_time_per_iteration * multiplier
        estimated_minutes = estimated_seconds / 60 + oom_search_time
        
        return estimated_minutes
    
    def save_results(self, output_file="benchmark_results.yaml"):
        """å„²å­˜æ¸¬è©¦çµæœ"""
        with open(output_file, 'w', encoding='utf-8') as f:
            yaml.dump(self.results, f, default_flow_style=False, allow_unicode=True)
        print(f"çµæœå·²å„²å­˜è‡³: {output_file}")
    
    def print_comprehensive_summary(self, gpu_type):
        """åˆ—å°æ“¬çœŸæ¸¬è©¦æ‘˜è¦"""
        if gpu_type not in self.results:
            return
            
        data = self.results[gpu_type]
        results = data['benchmark_results']
        
        print(f"\n{'='*80}")
        print(f"ğŸ† {gpu_type} æ“¬çœŸæ•ˆèƒ½æ¸¬è©¦å ±å‘Š")
        print(f"{'='*80}")
        
        # GPU åŸºæœ¬è³‡è¨Š
        gpu_info = data['gpu_info']
        sys_info = data['system_info']
        print(f"ğŸ–¥ï¸  GPU: {gpu_info['name']} ({gpu_info['memory_gb']}GB)")
        print(f"ğŸ’» ç³»çµ±: {sys_info['cpu_count']} æ ¸å¿ƒ, {sys_info['memory_gb']:.1f}GB RAM")
        print(f"ğŸ ç’°å¢ƒ: PyTorch {sys_info['pytorch_version']}, CUDA {sys_info['cuda_version']}")
        
        if data.get('max_batch_size_found'):
            print(f"ğŸ¯ æœ€å¤§ Batch Size: {data['max_batch_size_found']}")
        
        print(f"\n{'Batch':<8} {'Level':<8} {'Status':<8} {'Time/Batch':<12} {'Memory':<10} {'FPS':<8} {'GPU%':<6} {'Temp':<6}")
        print("-" * 80)
        
        # è©³ç´°çµæœè¡¨æ ¼
        best_fps = 0
        best_config = None
        
        for batch_size, batch_results in results.items():
            for level, result in batch_results.items():
                if result['successful']:
                    fps = result['fps']
                    if fps > best_fps:
                        best_fps = fps
                        best_config = (batch_size, level)
                    
                    print(f"{batch_size:<8} {level:<8} {'âœ…':<8} {result['avg_time_per_batch']:.3f}s{'':<6} "
                          f"{result['peak_memory_gb']:.1f}GB{'':<4} {fps:<8.0f} "
                          f"{result['gpu_utilization_avg']:.0f}%{'':<3} {result['gpu_temperature_max']:.0f}Â°C")
                else:
                    error_msg = result.get('error', 'æœªçŸ¥éŒ¯èª¤')[:10]
                    print(f"{batch_size:<8} {level:<8} {'âŒ':<8} {error_msg}")
        
        # æ•ˆèƒ½æ‘˜è¦
        print(f"\nğŸ† æœ€ä½³æ•ˆèƒ½é…ç½®:")
        if best_config:
            batch_size, level = best_config
            best_result = results[batch_size][level]
            print(f"   Batch Size: {batch_size} ({level} ç´šåˆ¥)")
            print(f"   æœ€é«˜ FPS: {best_fps:.0f}")
            print(f"   è¨˜æ†¶é«”ä½¿ç”¨: {best_result['peak_memory_gb']:.1f}GB")
            print(f"   GPU ä½¿ç”¨ç‡: {best_result['gpu_utilization_avg']:.0f}%")
    
    def generate_cross_gpu_comparison(self, gpu_types):
        """ç”Ÿæˆè·¨ GPU æ•ˆèƒ½æ¯”è¼ƒå ±å‘Š"""
        print(f"\n{'='*100}")
        print(f"ğŸ”¥ è·¨ GPU æ•ˆèƒ½æ¯”è¼ƒå ±å‘Š")
        print(f"{'='*100}")
        
        comparison_data = []
        for gpu_type in gpu_types:
            if gpu_type in self.results:
                data = self.results[gpu_type]
                
                # æ‰¾å‡ºæœ€ä½³æ•ˆèƒ½é…ç½®
                best_fps = 0
                best_batch = None
                max_batch = data.get('max_batch_size_found', 'N/A')
                
                for batch_size, batch_results in data['benchmark_results'].items():
                    for level, result in batch_results.items():
                        if result['successful'] and result['fps'] > best_fps:
                            best_fps = result['fps']
                            best_batch = batch_size
                
                comparison_data.append({
                    'gpu': gpu_type,
                    'memory_gb': data['gpu_info']['memory_gb'],
                    'best_fps': best_fps,
                    'best_batch': best_batch,
                    'max_batch': max_batch
                })
        
        # æ’åºä¸¦é¡¯ç¤º
        comparison_data.sort(key=lambda x: x['best_fps'], reverse=True)
        
        print(f"{'GPU':<10} {'è¨˜æ†¶é«”':<8} {'æœ€ä½³FPS':<10} {'æœ€ä½³Batch':<10} {'æ¥µé™Batch':<10} {'ç›¸å°æ•ˆèƒ½':<8}")
        print("-" * 70)
        
        baseline_fps = comparison_data[-1]['best_fps'] if comparison_data else 1
        
        for data in comparison_data:
            relative_perf = data['best_fps'] / baseline_fps
            print(f"{data['gpu']:<10} {data['memory_gb']}GB{'':<4} {data['best_fps']:<10.0f} "
                  f"{data['best_batch']:<10} {data['max_batch']:<10} {relative_perf:.1f}x")
        
        return comparison_data

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GPU æ“¬çœŸæ•ˆèƒ½æ¸¬è©¦ v2.0")
    parser.add_argument("--gpu-type", type=str, help="æŒ‡å®š GPU é¡å‹ (RTX4090, RTX5090, H100, B200)")
    parser.add_argument("--config", type=str, default="configs/gpu_configs.yaml", help="é…ç½®æª”æ¡ˆè·¯å¾‘")
    parser.add_argument("--output", type=str, default="benchmark_results_v2.yaml", help="è¼¸å‡ºæª”æ¡ˆ")
    parser.add_argument("--test-levels", nargs='+', default=['light', 'medium', 'heavy'], 
                        help="æ¸¬è©¦ç´šåˆ¥ (light, medium, heavy)")
    parser.add_argument("--find-limit", action='store_true', default=True, help="å°‹æ‰¾æœ€å¤§ batch size")
    parser.add_argument("--quick", action='store_true', help="å¿«é€Ÿæ¸¬è©¦ (åƒ… light ç´šåˆ¥)")
    parser.add_argument("--compare", nargs='+', help="æ¯”è¼ƒå¤šå€‹ GPU é¡å‹ (éœ€è¦å…ˆåŸ·è¡Œå„åˆ¥æ¸¬è©¦)")
    
    args = parser.parse_args()
    
    # å¿«é€Ÿæ¸¬è©¦æ¨¡å¼
    if args.quick:
        args.test_levels = ['light']
        args.find_limit = False
    
    benchmark = GPUBenchmark(args.config)
    
    # æ¯”è¼ƒæ¨¡å¼
    if args.compare:
        print("ğŸ”„ è¼‰å…¥ä¹‹å‰çš„æ¸¬è©¦çµæœé€²è¡Œæ¯”è¼ƒ...")
        try:
            with open(args.output, 'r', encoding='utf-8') as f:
                benchmark.results = yaml.safe_load(f)
            benchmark.generate_cross_gpu_comparison(args.compare)
        except FileNotFoundError:
            print("âŒ æ‰¾ä¸åˆ°ä¹‹å‰çš„æ¸¬è©¦çµæœï¼Œè«‹å…ˆåŸ·è¡Œå„ GPU çš„æ¸¬è©¦")
    else:
        # å–®ä¸€ GPU æ¸¬è©¦
        detected_gpu = benchmark.detect_gpu()
        target_gpu = args.gpu_type or detected_gpu
        
        print(f"ğŸ® é–‹å§‹ {target_gpu} æ“¬çœŸæ•ˆèƒ½æ¸¬è©¦...")
        results = benchmark.run_comprehensive_benchmark(
            target_gpu, 
            args.test_levels, 
            args.find_limit
        )
        
        benchmark.print_comprehensive_summary(target_gpu)
        benchmark.save_results(args.output)
        
        print(f"\nâœ… æ¸¬è©¦å®Œæˆï¼çµæœå·²å„²å­˜è‡³ {args.output}")
        print(f"ğŸ’¡ ä½¿ç”¨ --compare åƒæ•¸å¯æ¯”è¼ƒå¤šå€‹ GPU çµæœ")