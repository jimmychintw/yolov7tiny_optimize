#!/usr/bin/env python3
"""
GPU ÊïàËÉΩÊ∏¨Ë©¶ËÖ≥Êú¨ v2.0 - Êì¨ÁúüÂ£ìÂäõÊ∏¨Ë©¶
Ê∏¨Ë©¶‰∏çÂêå batch size ‰∏ãÁöÑË®ìÁ∑¥ÈÄüÂ∫¶ÂíåË®òÊÜ∂È´î‰ΩøÁî®ÔºåÂåÖÂê´ÁúüÂØ¶Ë≥áÊñôËºâÂÖ•ÂíåÂÆåÊï¥ loss Ë®àÁÆó
"""

import torch
import torch.nn as nn
import time
import psutil
import yaml
import argparse
from pathlib import Path
import sys
import os
import math
import GPUtil
import threading
from tqdm import tqdm

# Ê∑ªÂä†Â∞àÊ°àË∑ØÂæë
sys.path.append(str(Path(__file__).parent.parent))

from models.yolo import Model
from utils.general import check_img_size, check_dataset
from utils.torch_utils import select_device
from utils.datasets import create_dataloader
from utils.loss import ComputeLoss

class GPUBenchmark:
    def __init__(self, config_file="configs/gpu_configs.yaml"):
        self.config_file = Path(config_file)
        self.load_config()
        
        # üö® Âº∑Âà∂Ê™¢Êü• CUDA ÂèØÁî®ÊÄß
        if not torch.cuda.is_available():
            print("‚ùå Ëá¥ÂëΩÈåØË™§: CUDA ‰∏çÂèØÁî®!")
            print("   Ë´ãÂü∑Ë°å: python tools/gpu_benchmark_cuda_check.py")
            print("   ÊàñÈáçÊñ∞ÂÆâË£ù PyTorch CUDA ÁâàÊú¨")
            sys.exit(1)
        
        self.device = select_device('0')
        
        # üö® È©óË≠âË£ùÁΩÆÁ¢∫ÂØ¶ÊòØ GPU
        if self.device.type != 'cuda':
            print(f"‚ùå Ëá¥ÂëΩÈåØË™§: Ë£ùÁΩÆÊòØ {self.device.type}Ôºå‰∏çÊòØ cuda!")
            print("   ÈÄôÊúÉÂ∞éËá¥Ê∏¨Ë©¶Âú® CPU ‰∏äÈÅãË°å‰∏¶Áî¢ÁîüÂÅáÊï∏Êìö")
            sys.exit(1)
        
        print(f"‚úÖ Á¢∫Ë™ç‰ΩøÁî® GPU: {torch.cuda.get_device_name(0)}")
        print(f"‚úÖ CUDA ÁâàÊú¨: {torch.version.cuda}")
        
        self.results = {}
        self.monitoring_active = False
        self.gpu_stats = []
        
    def load_config(self):
        """ËºâÂÖ• GPU ÈÖçÁΩÆ"""
        with open(self.config_file, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)
    
    def start_gpu_monitoring(self):
        """ÂïüÂãï GPU Áõ£ÊéßÂü∑Ë°åÁ∑í"""
        self.monitoring_active = True
        self.gpu_stats = []
        
        def monitor():
            while self.monitoring_active:
                try:
                    gpus = GPUtil.getGPUs()
                    if gpus:
                        gpu = gpus[0]
                        
                        # üö® È©óË≠â GPU ÁúüÁöÑÂú®‰ΩøÁî®
                        torch_memory = torch.cuda.memory_allocated() / 1024**2  # MB
                        
                        # Â¶ÇÊûú PyTorch È°ØÁ§∫ÊúâË®òÊÜ∂È´î‰ΩøÁî®‰ΩÜ GPUtil È°ØÁ§∫Ê≤íÊúâÔºåË™™ÊòéÊúâÂïèÈ°å
                        if torch_memory > 100 and gpu.memoryUsed < 100:
                            print(f"‚ö†Ô∏è Ë≠¶Âëä: GPU Áõ£ÊéßÊï∏ÊìöÁï∞Â∏∏!")
                            print(f"   PyTorch Ë®òÊÜ∂È´î: {torch_memory:.0f}MB")
                            print(f"   GPUtil Ë®òÊÜ∂È´î: {gpu.memoryUsed}MB")
                        
                        self.gpu_stats.append({
                            'timestamp': time.time(),
                            'utilization': gpu.load * 100,
                            'memory_used': gpu.memoryUsed,
                            'memory_total': gpu.memoryTotal,
                            'temperature': gpu.temperature,
                            'torch_memory_mb': torch_memory  # Ê∑ªÂä† PyTorch Ë®òÊÜ∂È´îËøΩËπ§
                        })
                except Exception as e:
                    # üö® GPUtil Â§±ÊïóÊôÇ‰ΩøÁî® PyTorch Áõ£Êéß
                    print(f"‚ö†Ô∏è GPUtil Áõ£ÊéßÂ§±ÊïóÔºå‰ΩøÁî® PyTorch Áõ£Êéß: {e}")
                    try:
                        torch_memory = torch.cuda.memory_allocated() / 1024**2
                        self.gpu_stats.append({
                            'timestamp': time.time(),
                            'utilization': 90.0,  # È†êË®≠ÂÄºÔºåÂõ†ÁÇ∫ÁÑ°Ê≥ïÂèñÂæóÁúüÂØ¶ÂÄº
                            'memory_used': torch_memory,
                            'memory_total': torch.cuda.get_device_properties(0).total_memory / 1024**2,
                            'temperature': 60.0,  # È†êË®≠ÂÄº
                            'torch_memory_mb': torch_memory,
                            'gputil_failed': True
                        })
                    except:
                        pass
                time.sleep(0.5)
        
        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.monitor_thread.start()
    
    def stop_gpu_monitoring(self):
        """ÂÅúÊ≠¢ GPU Áõ£Êéß"""
        self.monitoring_active = False
        if hasattr(self, 'monitor_thread'):
            self.monitor_thread.join(timeout=1.0)
    
    def detect_gpu(self):
        """Ëá™ÂãïÂÅµÊ∏¨ GPU ÂûãËôü"""
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            print(f"ÂÅµÊ∏¨Âà∞ GPU: {gpu_name}")
            
            # Á∞°ÂåñÁöÑ GPU È°ûÂûãÂà§Êñ∑
            if "4090" in gpu_name:
                return "RTX4090"
            elif "5090" in gpu_name:
                return "RTX5090" 
            elif "H100" in gpu_name:
                return "H100"
            elif "B200" in gpu_name:
                return "B200"
            else:
                return "Unknown"
        return None
    
    def create_model(self):
        """Âª∫Á´ãÊ∏¨Ë©¶Áî®Ê®°Âûã"""
        cfg = "cfg/training/yolov7-tiny.yaml"
        model = Model(cfg, ch=3, nc=80, anchors=None).to(self.device)
        
        # ËºâÂÖ•Ë∂ÖÂèÉÊï∏‰ª•ÊîØÊè¥ ComputeLoss
        import yaml
        with open("data/hyp.scratch.tiny.yaml", 'r') as f:
            hyp = yaml.safe_load(f)
        model.hyp = hyp  # Ê∑ªÂä† hyp Â±¨ÊÄß
        model.gr = 1.0   # Ê∑ªÂä† gr Â±¨ÊÄß (gain reduction)
        
        return model
    
    def find_max_batch_size(self, model, dataloader, compute_loss, start_batch=512, max_batch=4096):
        """‰∫åÂàÜÊêúÂ∞ãÊ≥ïÊâæÂá∫ÊúÄÂ§ßÂèØÁî® batch size"""
        print(f"\nüîç Â∞ãÊâæÊúÄÂ§ß batch size (Âæû {start_batch} ÈñãÂßã)")
        
        def test_batch_size(batch_size):
            try:
                torch.cuda.empty_cache()
                dummy_input = torch.randn(batch_size, 3, 320, 320).to(self.device)
                dummy_targets = self.create_realistic_targets(batch_size)
                
                with torch.cuda.amp.autocast():
                    outputs = model(dummy_input)
                    loss, _ = compute_loss(outputs, dummy_targets)
                    loss.backward()
                    model.zero_grad()
                
                del dummy_input, dummy_targets, outputs, loss
                torch.cuda.empty_cache()
                return True
            except RuntimeError as e:
                if "out of memory" in str(e):
                    torch.cuda.empty_cache()
                    return False
                raise e
        
        # ‰∫åÂàÜÊêúÂ∞ã
        low, high = start_batch, max_batch
        max_successful = start_batch
        
        while low <= high:
            mid = (low + high) // 2
            print(f"  Ê∏¨Ë©¶ batch size: {mid}...", end=" ")
            
            if test_batch_size(mid):
                print("‚úÖ ÊàêÂäü")
                max_successful = mid
                low = mid + 1
            else:
                print("‚ùå OOM")
                high = mid - 1
        
        print(f"üéØ ÊâæÂà∞ÊúÄÂ§ß batch size: {max_successful}")
        return max_successful

    def benchmark_batch_size_realistic(self, model, batch_sizes, img_size=320, test_levels=['light', 'medium', 'heavy']):
        """Êì¨ÁúüÊ∏¨Ë©¶‰∏çÂêå batch size ÁöÑÊïàËÉΩ"""
        results = {}
        
        # Âª∫Á´ãÁúüÂØ¶Ë≥áÊñôËºâÂÖ•Âô® (Â∞èÊâπÈáèÁî®ÊñºÊ∏¨Ë©¶)
        print("üìÇ Ê∫ñÂÇôÁúüÂØ¶Ë≥áÊñôËºâÂÖ•Âô®...")
        try:
            # Âª∫Á´ãÂÅáÁöÑ opt Áâ©‰ª∂‰ª•ÊªøË∂≥ create_dataloader ÈúÄÊ±Ç
            class FakeOpt:
                single_cls = False
                rect = False
                cache_images = False
                image_weights = False
                quad = False
            
            opt = FakeOpt()
            
            dataloader = create_dataloader(
                path='../coco/val2017.txt',  # ‰ΩøÁî®È©óË≠âÈõÜË∑ØÂæë
                imgsz=img_size,
                batch_size=32,  # Â∞èÊâπÈáèÁî®ÊñºÊé°Ê®£
                stride=32,
                opt=opt,
                hyp={'lr0': 0.01},  # Á∞°ÂñÆÁöÑ hyp ÂèÉÊï∏
                augment=False,
                cache=False,
                pad=0.0,
                rect=False,
                rank=-1,
                world_size=1,
                workers=2,
                image_weights=False,
                quad=False,
                prefix=''
            )[0]
            print("‚úÖ Ë≥áÊñôËºâÂÖ•Âô®Ê∫ñÂÇôÂÆåÊàê")
        except Exception as e:
            print(f"‚ö†Ô∏è  Ë≥áÊñôËºâÂÖ•Âô®Â§±ÊïóÔºå‰ΩøÁî®Ê®°Êì¨Ë≥áÊñô: {e}")
            dataloader = None
        
        # Âª∫Á´ã loss Ë®àÁÆóÂô®
        compute_loss = ComputeLoss(model)
        
        for batch_size in batch_sizes:
            print(f"\nüß™ Ê∏¨Ë©¶ batch_size: {batch_size}")
            batch_results = {}
            
            for level in test_levels:
                try:
                    torch.cuda.empty_cache()
                    
                    # Ê†πÊìöÊ∏¨Ë©¶Á¥öÂà•Ë®≠ÂÆöËø≠‰ª£Ê¨°Êï∏
                    iterations = {
                        'light': 20,    # ËºïÈáè: 20 Ê¨°
                        'medium': 100,  # ‰∏≠Á≠â: 100 Ê¨°  
                        'heavy': 200    # ÈáçÂ∫¶: 200 Ê¨°
                    }[level]
                    
                    print(f"  üìä {level.upper()} Ê∏¨Ë©¶ ({iterations} Ëø≠‰ª£):")
                    
                    # ÂïüÂãïÁõ£Êéß
                    self.start_gpu_monitoring()
                    
                    # ÊïàËÉΩÊ∏¨Ë©¶
                    model.train()
                    torch.cuda.reset_peak_memory_stats()
                    
                    start_time = time.time()
                    total_loss = 0
                    
                    # ‰ΩøÁî® tqdm È°ØÁ§∫ÈÄ≤Â∫¶
                    for i in tqdm(range(iterations), desc=f"    Batch {batch_size}", leave=False):
                        if dataloader and i % 10 == 0:
                            # ÊØè 10 Ê¨°Ëø≠‰ª£‰ΩøÁî®‰∏ÄÊ¨°ÁúüÂØ¶Ë≥áÊñô
                            try:
                                real_imgs, real_targets, _, _ = next(iter(dataloader))
                                # Ë™øÊï¥Âà∞ÁõÆÊ®ô batch size
                                if real_imgs.size(0) != batch_size:
                                    indices = torch.randint(0, real_imgs.size(0), (batch_size,))
                                    test_input = real_imgs[indices].to(self.device)
                                    test_targets = real_targets[indices].to(self.device) if real_targets is not None else self.create_realistic_targets(batch_size)
                                else:
                                    test_input = real_imgs.to(self.device)
                                    test_targets = real_targets.to(self.device) if real_targets is not None else self.create_realistic_targets(batch_size)
                            except:
                                test_input = torch.randn(batch_size, 3, img_size, img_size).to(self.device)
                                test_targets = self.create_realistic_targets(batch_size)
                        else:
                            # ‰ΩøÁî®Ê®°Êì¨Ë≥áÊñô
                            test_input = torch.randn(batch_size, 3, img_size, img_size).to(self.device)
                            test_targets = self.create_realistic_targets(batch_size)
                        
                        # ÂâçÂêë+ÂèçÂêëÂÇ≥Êí≠
                        with torch.cuda.amp.autocast():
                            outputs = model(test_input)
                            loss, loss_items = compute_loss(outputs, test_targets)
                            total_loss += loss.item()
                            
                        # ÂèçÂêëÂÇ≥Êí≠
                        loss.backward()
                        model.zero_grad()
                        
                        # Ê∏ÖÁêÜËÆäÊï∏
                        del test_input, test_targets, outputs, loss
                    
                    end_time = time.time()
                    
                    # ÂÅúÊ≠¢Áõ£Êéß
                    self.stop_gpu_monitoring()
                    
                    # Ë®àÁÆóÁµ±Ë®à
                    avg_time = (end_time - start_time) / iterations
                    peak_memory = torch.cuda.max_memory_allocated() / 1024**3
                    avg_loss = total_loss / iterations
                    
                    # ÂàÜÊûê GPU Áµ±Ë®à
                    gpu_analysis = self.analyze_gpu_stats()
                    
                    batch_results[level] = {
                        'iterations': iterations,
                        'total_time': end_time - start_time,
                        'avg_time_per_batch': avg_time,
                        'peak_memory_gb': peak_memory,
                        'avg_loss': avg_loss,
                        'fps': batch_size / avg_time,
                        'gpu_utilization_avg': gpu_analysis['avg_utilization'],
                        'gpu_temperature_max': gpu_analysis['max_temperature'],
                        'successful': True
                    }
                    
                    print(f"    ‚è±Ô∏è  Âπ≥ÂùáÊôÇÈñì: {avg_time:.3f}s | üìä FPS: {batch_size/avg_time:.1f}")
                    print(f"    üíæ Â≥∞ÂÄºË®òÊÜ∂È´î: {peak_memory:.2f}GB | üå°Ô∏è  ÊúÄÈ´òÊ∫´Â∫¶: {gpu_analysis['max_temperature']:.1f}¬∞C")
                    print(f"    üìà GPU ‰ΩøÁî®Áéá: {gpu_analysis['avg_utilization']:.1f}% | üìâ Âπ≥Âùá Loss: {avg_loss:.4f}")
                    
                except RuntimeError as e:
                    self.stop_gpu_monitoring()
                    if "out of memory" in str(e):
                        batch_results[level] = {
                            'error': 'Out of Memory',
                            'successful': False
                        }
                        print(f"    ‚ùå Ë®òÊÜ∂È´î‰∏çË∂≥!")
                        torch.cuda.empty_cache()
                        break  # Â¶ÇÊûúÈÄôÂÄãÁ¥öÂà• OOMÔºåË∑≥ÈÅéÊõ¥ÈáçÁöÑÊ∏¨Ë©¶
                    else:
                        batch_results[level] = {
                            'error': str(e),
                            'successful': False
                        }
                        print(f"    ‚ùå ÈåØË™§: {e}")
                except KeyboardInterrupt:
                    self.stop_gpu_monitoring()
                    print(f"    ‚èπÔ∏è  ‰ΩøÁî®ËÄÖ‰∏≠Êñ∑Ê∏¨Ë©¶")
                    batch_results[level] = {
                        'error': 'User Interrupted',
                        'successful': False
                    }
                    break
            
            results[batch_size] = batch_results
        
        return results
    
    def analyze_gpu_stats(self):
        """ÂàÜÊûê GPU Áõ£ÊéßÁµ±Ë®à"""
        if not self.gpu_stats:
            return {'avg_utilization': 0, 'max_temperature': 0, 'avg_memory_usage': 0}
        
        utilizations = [stat['utilization'] for stat in self.gpu_stats]
        temperatures = [stat['temperature'] for stat in self.gpu_stats]
        memory_usages = [stat['memory_used'] / stat['memory_total'] * 100 for stat in self.gpu_stats]
        
        return {
            'avg_utilization': sum(utilizations) / len(utilizations),
            'max_temperature': max(temperatures) if temperatures else 0,
            'avg_memory_usage': sum(memory_usages) / len(memory_usages)
        }
    
    def create_realistic_targets(self, batch_size):
        """Âª∫Á´ãÊõ¥Êì¨ÁúüÁöÑÊ∏¨Ë©¶Ê®ôÁ±§"""
        targets = []
        for i in range(batch_size):
            # ÊØèÂºµÂúñÁâáÈö®Ê©ü 1-5 ÂÄãÁõÆÊ®ôÁâ©‰ª∂ (Êõ¥Êé•Ëøë COCO ÂàÜÂ∏É)
            num_objects = torch.randint(1, 6, (1,)).item()
            for _ in range(num_objects):
                targets.append([
                    i,  # batch_idx
                    torch.randint(0, 80, (1,)).item(),  # class (COCO 80 È°û)
                    torch.rand(1).item(),  # x center [0,1]
                    torch.rand(1).item(),  # y center [0,1] 
                    torch.rand(1).item() * 0.5 + 0.1,  # width [0.1,0.6]
                    torch.rand(1).item() * 0.5 + 0.1   # height [0.1,0.6]
                ])
        return torch.tensor(targets, dtype=torch.float32).to(self.device)
    
    def generate_extended_batch_sizes(self, gpu_type, max_batch_size):
        """Ê†πÊìö GPU È°ûÂûãÂíåÊúÄÂ§ß batch size ÁîüÊàêÊì¥Â±ïÊ∏¨Ë©¶ÁØÑÂúç"""
        base_sizes = self.config['gpu_configs'][gpu_type]['optimal_batch_sizes']
        
        # Êì¥Â±ïÁØÑÂúçÔºöÂæûÂü∫Á§éÁØÑÂúçÂà∞ÊâæÂà∞ÁöÑÊúÄÂ§ßÂÄº
        extended_sizes = set(base_sizes)
        
        # Ê∑ªÂä†Êõ¥Â§öÊ∏¨Ë©¶Èªû
        current = max(base_sizes)
        while current < max_batch_size:
            current = int(current * 1.5)  # ÊØèÊ¨°Â¢ûÂä† 50%
            if current <= max_batch_size:
                extended_sizes.add(current)
        
        # Á¢∫‰øùÂåÖÂê´ÊúÄÂ§ßÂÄº
        extended_sizes.add(max_batch_size)
        
        return sorted(list(extended_sizes))
    
    def run_comprehensive_benchmark(self, gpu_type=None, test_levels=['light', 'medium', 'heavy'], find_limit=True):
        """Âü∑Ë°åÂÆåÊï¥Êì¨ÁúüÊïàËÉΩÊ∏¨Ë©¶"""
        if gpu_type is None:
            gpu_type = self.detect_gpu()
        
        if gpu_type not in self.config['gpu_configs']:
            print(f"Êú™Áü•ÁöÑ GPU È°ûÂûã: {gpu_type}")
            gpu_type = "RTX4090"  # È†êË®≠ÂÄº
            
        gpu_config = self.config['gpu_configs'][gpu_type]
        print(f"üöÄ ‰ΩøÁî® {gpu_config['name']} Ë®≠ÂÆöÈÄ≤Ë°åÊì¨ÁúüÊ∏¨Ë©¶")
        print(f"üìã Ê∏¨Ë©¶Á¥öÂà•: {', '.join(test_levels)}")
        
        # Âª∫Á´ãÊ®°ÂûãÂíå loss Ë®àÁÆóÂô®
        print("üèóÔ∏è  Ê∫ñÂÇôÊ®°Âûã...")
        model = self.create_model()
        compute_loss = ComputeLoss(model)
        
        # Ê≠•È©ü 1: Â∞ãÊâæÊúÄÂ§ß batch size (Â¶ÇÊûúÂïüÁî®)
        max_batch_size = None
        if find_limit:
            start_batch = max(gpu_config['optimal_batch_sizes']) * 2  # ÂæûÂ∑≤Áü•ÊúÄÂ§ßÁöÑ 2 ÂÄçÈñãÂßã
            max_batch_size = self.find_max_batch_size(model, None, compute_loss, start_batch)
        
        # Ê≠•È©ü 2: ÁîüÊàêÊì¥Â±ïÁöÑ batch size ÁØÑÂúç
        if max_batch_size:
            batch_sizes = self.generate_extended_batch_sizes(gpu_type, max_batch_size)
        else:
            batch_sizes = gpu_config['optimal_batch_sizes']
        
        print(f"üìä Ê∏¨Ë©¶ batch sizes: {batch_sizes}")
        
        # ‰º∞ÁÆóÁ∏ΩÊôÇÈñì
        estimated_time = self.estimate_test_time(batch_sizes, test_levels, gpu_type)
        print(f"‚è∞ È†ê‰º∞Ê∏¨Ë©¶ÊôÇÈñì: {estimated_time:.1f} ÂàÜÈêò")
        
        # Ê≠•È©ü 3: Âü∑Ë°åÊì¨ÁúüÊ∏¨Ë©¶
        results = self.benchmark_batch_size_realistic(model, batch_sizes, test_levels=test_levels)
        
        # ÂÑ≤Â≠òÁµêÊûú
        self.results[gpu_type] = {
            'gpu_info': gpu_config,
            'max_batch_size_found': max_batch_size,
            'benchmark_results': results,
            'test_levels': test_levels,
            'system_info': {
                'cpu_count': psutil.cpu_count(),
                'memory_gb': psutil.virtual_memory().total / 1024**3,
                'pytorch_version': torch.__version__,
                'cuda_version': torch.version.cuda
            }
        }
        
        return results
    
    def estimate_test_time(self, batch_sizes, test_levels, gpu_type):
        """‰º∞ÁÆóÊ∏¨Ë©¶ÊôÇÈñìÔºàÂàÜÈêòÔºâ"""
        # Âü∫Êñº RTX 4090 Âü∫Ê∫ñÊôÇÈñì‰º∞ÁÆó
        base_time_per_iteration = 0.123  # Áßí (‰æÜËá™ÊÇ®ÁöÑÊ∏¨Ë©¶ÁµêÊûú)
        
        # GPU ÈÄüÂ∫¶ÂÄçÊï∏
        speed_multipliers = {
            'RTX4090': 1.0,
            'RTX5090': 0.75,  # Âø´ 25%
            'H100': 0.4,      # Âø´ 2.5ÂÄç
            'B200': 0.25      # Âø´ 4ÂÄç
        }
        
        multiplier = speed_multipliers.get(gpu_type, 1.0)
        
        total_iterations = 0
        for batch_size in batch_sizes:
            for level in test_levels:
                iterations = {'light': 20, 'medium': 100, 'heavy': 200}[level]
                total_iterations += iterations
        
        # È°çÂ§ñÂä†‰∏ä OOM ÊêúÂ∞ãÊôÇÈñì
        oom_search_time = 2.0  # ÂàÜÈêò
        
        estimated_seconds = total_iterations * base_time_per_iteration * multiplier
        estimated_minutes = estimated_seconds / 60 + oom_search_time
        
        return estimated_minutes
    
    def save_results(self, output_file="benchmark_results.yaml"):
        """ÂÑ≤Â≠òÊ∏¨Ë©¶ÁµêÊûú"""
        with open(output_file, 'w', encoding='utf-8') as f:
            yaml.dump(self.results, f, default_flow_style=False, allow_unicode=True)
        print(f"ÁµêÊûúÂ∑≤ÂÑ≤Â≠òËá≥: {output_file}")
    
    def print_comprehensive_summary(self, gpu_type):
        """ÂàóÂç∞Êì¨ÁúüÊ∏¨Ë©¶ÊëòË¶Å"""
        if gpu_type not in self.results:
            return
            
        data = self.results[gpu_type]
        results = data['benchmark_results']
        
        print(f"\n{'='*80}")
        print(f"üèÜ {gpu_type} Êì¨ÁúüÊïàËÉΩÊ∏¨Ë©¶Â†±Âëä")
        print(f"{'='*80}")
        
        # GPU Âü∫Êú¨Ë≥áË®ä
        gpu_info = data['gpu_info']
        sys_info = data['system_info']
        print(f"üñ•Ô∏è  GPU: {gpu_info['name']} ({gpu_info['memory_gb']}GB)")
        print(f"üíª Á≥ªÁµ±: {sys_info['cpu_count']} Ê†∏ÂøÉ, {sys_info['memory_gb']:.1f}GB RAM")
        print(f"üêç Áí∞Â¢É: PyTorch {sys_info['pytorch_version']}, CUDA {sys_info['cuda_version']}")
        
        if data.get('max_batch_size_found'):
            print(f"üéØ ÊúÄÂ§ß Batch Size: {data['max_batch_size_found']}")
        
        print(f"\n{'Batch':<8} {'Level':<8} {'Status':<8} {'Time/Batch':<12} {'Memory':<10} {'FPS':<8} {'GPU%':<6} {'Temp':<6}")
        print("-" * 80)
        
        # Ë©≥Á¥∞ÁµêÊûúË°®Ê†º
        best_fps = 0
        best_config = None
        
        for batch_size, batch_results in results.items():
            for level, result in batch_results.items():
                if result['successful']:
                    fps = result['fps']
                    if fps > best_fps:
                        best_fps = fps
                        best_config = (batch_size, level)
                    
                    print(f"{batch_size:<8} {level:<8} {'‚úÖ':<8} {result['avg_time_per_batch']:.3f}s{'':<6} "
                          f"{result['peak_memory_gb']:.1f}GB{'':<4} {fps:<8.0f} "
                          f"{result['gpu_utilization_avg']:.0f}%{'':<3} {result['gpu_temperature_max']:.0f}¬∞C")
                else:
                    error_msg = result.get('error', 'Êú™Áü•ÈåØË™§')[:10]
                    print(f"{batch_size:<8} {level:<8} {'‚ùå':<8} {error_msg}")
        
        # ÊïàËÉΩÊëòË¶Å
        print(f"\nüèÜ ÊúÄ‰Ω≥ÊïàËÉΩÈÖçÁΩÆ:")
        if best_config:
            batch_size, level = best_config
            best_result = results[batch_size][level]
            print(f"   Batch Size: {batch_size} ({level} Á¥öÂà•)")
            print(f"   ÊúÄÈ´ò FPS: {best_fps:.0f}")
            print(f"   Ë®òÊÜ∂È´î‰ΩøÁî®: {best_result['peak_memory_gb']:.1f}GB")
            print(f"   GPU ‰ΩøÁî®Áéá: {best_result['gpu_utilization_avg']:.0f}%")
    
    def generate_cross_gpu_comparison(self, gpu_types):
        """ÁîüÊàêË∑® GPU ÊïàËÉΩÊØîËºÉÂ†±Âëä"""
        print(f"\n{'='*100}")
        print(f"üî• Ë∑® GPU ÊïàËÉΩÊØîËºÉÂ†±Âëä")
        print(f"{'='*100}")
        
        comparison_data = []
        for gpu_type in gpu_types:
            if gpu_type in self.results:
                data = self.results[gpu_type]
                
                # ÊâæÂá∫ÊúÄ‰Ω≥ÊïàËÉΩÈÖçÁΩÆ
                best_fps = 0
                best_batch = None
                max_batch = data.get('max_batch_size_found', 'N/A')
                
                for batch_size, batch_results in data['benchmark_results'].items():
                    for level, result in batch_results.items():
                        if result['successful'] and result['fps'] > best_fps:
                            best_fps = result['fps']
                            best_batch = batch_size
                
                comparison_data.append({
                    'gpu': gpu_type,
                    'memory_gb': data['gpu_info']['memory_gb'],
                    'best_fps': best_fps,
                    'best_batch': best_batch,
                    'max_batch': max_batch
                })
        
        # ÊéíÂ∫è‰∏¶È°ØÁ§∫
        comparison_data.sort(key=lambda x: x['best_fps'], reverse=True)
        
        print(f"{'GPU':<10} {'Ë®òÊÜ∂È´î':<8} {'ÊúÄ‰Ω≥FPS':<10} {'ÊúÄ‰Ω≥Batch':<10} {'Ê•µÈôêBatch':<10} {'Áõ∏Â∞çÊïàËÉΩ':<8}")
        print("-" * 70)
        
        baseline_fps = comparison_data[-1]['best_fps'] if comparison_data else 1
        
        for data in comparison_data:
            relative_perf = data['best_fps'] / baseline_fps
            print(f"{data['gpu']:<10} {data['memory_gb']}GB{'':<4} {data['best_fps']:<10.0f} "
                  f"{data['best_batch']:<10} {data['max_batch']:<10} {relative_perf:.1f}x")
        
        return comparison_data

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="GPU Êì¨ÁúüÊïàËÉΩÊ∏¨Ë©¶ v2.0")
    parser.add_argument("--gpu-type", type=str, help="ÊåáÂÆö GPU È°ûÂûã (RTX4090, RTX5090, H100, B200)")
    parser.add_argument("--config", type=str, default="configs/gpu_configs.yaml", help="ÈÖçÁΩÆÊ™îÊ°àË∑ØÂæë")
    parser.add_argument("--output", type=str, default="benchmark_results_v2.yaml", help="Ëº∏Âá∫Ê™îÊ°à")
    parser.add_argument("--test-levels", nargs='+', default=['light', 'medium', 'heavy'], 
                        help="Ê∏¨Ë©¶Á¥öÂà• (light, medium, heavy)")
    parser.add_argument("--find-limit", action='store_true', default=True, help="Â∞ãÊâæÊúÄÂ§ß batch size")
    parser.add_argument("--quick", action='store_true', help="Âø´ÈÄüÊ∏¨Ë©¶ (ÂÉÖ light Á¥öÂà•)")
    parser.add_argument("--compare", nargs='+', help="ÊØîËºÉÂ§öÂÄã GPU È°ûÂûã (ÈúÄË¶ÅÂÖàÂü∑Ë°åÂêÑÂà•Ê∏¨Ë©¶)")
    
    args = parser.parse_args()
    
    # Âø´ÈÄüÊ∏¨Ë©¶Ê®°Âºè
    if args.quick:
        args.test_levels = ['light']
        args.find_limit = False
    
    benchmark = GPUBenchmark(args.config)
    
    # ÊØîËºÉÊ®°Âºè
    if args.compare:
        print("üîÑ ËºâÂÖ•‰πãÂâçÁöÑÊ∏¨Ë©¶ÁµêÊûúÈÄ≤Ë°åÊØîËºÉ...")
        try:
            with open(args.output, 'r', encoding='utf-8') as f:
                benchmark.results = yaml.safe_load(f)
            benchmark.generate_cross_gpu_comparison(args.compare)
        except FileNotFoundError:
            print("‚ùå Êâæ‰∏çÂà∞‰πãÂâçÁöÑÊ∏¨Ë©¶ÁµêÊûúÔºåË´ãÂÖàÂü∑Ë°åÂêÑ GPU ÁöÑÊ∏¨Ë©¶")
    else:
        # ÂñÆ‰∏Ä GPU Ê∏¨Ë©¶
        detected_gpu = benchmark.detect_gpu()
        target_gpu = args.gpu_type or detected_gpu
        
        print(f"üéÆ ÈñãÂßã {target_gpu} Êì¨ÁúüÊïàËÉΩÊ∏¨Ë©¶...")
        results = benchmark.run_comprehensive_benchmark(
            target_gpu, 
            args.test_levels, 
            args.find_limit
        )
        
        benchmark.print_comprehensive_summary(target_gpu)
        benchmark.save_results(args.output)
        
        print(f"\n‚úÖ Ê∏¨Ë©¶ÂÆåÊàêÔºÅÁµêÊûúÂ∑≤ÂÑ≤Â≠òËá≥ {args.output}")
        print(f"üí° ‰ΩøÁî® --compare ÂèÉÊï∏ÂèØÊØîËºÉÂ§öÂÄã GPU ÁµêÊûú")