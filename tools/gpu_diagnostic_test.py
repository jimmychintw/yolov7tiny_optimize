#!/usr/bin/env python3
"""
GPU è¨ºæ–·æ¸¬è©¦ - ç¢ºèª GPU æ˜¯å¦çœŸæ­£åœ¨å·¥ä½œ
"""

import torch
import time
import subprocess
import numpy as np

def run_command(cmd):
    """åŸ·è¡Œå‘½ä»¤ä¸¦è¿”å›è¼¸å‡º"""
    try:
        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
        return result.stdout
    except:
        return "Command failed"

def gpu_stress_test():
    """GPU å£“åŠ›æ¸¬è©¦ - ç¢ºä¿ GPU çœŸçš„åœ¨å·¥ä½œ"""
    print("=" * 60)
    print("ğŸ”¬ GPU è¨ºæ–·æ¸¬è©¦é–‹å§‹")
    print("=" * 60)
    
    # 1. åŸºæœ¬ CUDA æª¢æŸ¥
    print("\n1ï¸âƒ£ åŸºæœ¬ CUDA æª¢æŸ¥:")
    print(f"   CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"   GPU æ•¸é‡: {torch.cuda.device_count()}")
        print(f"   ç•¶å‰ GPU: {torch.cuda.current_device()}")
        print(f"   GPU åç¨±: {torch.cuda.get_device_name(0)}")
        print(f"   CUDA ç‰ˆæœ¬: {torch.version.cuda}")
        
        # æª¢æŸ¥ GPU å±¬æ€§
        props = torch.cuda.get_device_properties(0)
        print(f"   è¨ˆç®—èƒ½åŠ›: {props.major}.{props.minor}")
        print(f"   è¨˜æ†¶é«”: {props.total_memory / 1024**3:.1f} GB")
    else:
        print("   âŒ CUDA ä¸å¯ç”¨ï¼åœæ­¢æ¸¬è©¦")
        return
    
    # 2. æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
    print("\n2ï¸âƒ£ ç’°å¢ƒè®Šæ•¸æª¢æŸ¥:")
    import os
    cuda_visible = os.environ.get('CUDA_VISIBLE_DEVICES', 'æœªè¨­å®š')
    print(f"   CUDA_VISIBLE_DEVICES: {cuda_visible}")
    
    # 3. åŸ·è¡Œå‰çš„ nvidia-smi
    print("\n3ï¸âƒ£ æ¸¬è©¦å‰ nvidia-smi:")
    before_smi = run_command("nvidia-smi --query-gpu=utilization.gpu,memory.used,temperature.gpu,power.draw --format=csv,noheader")
    print(f"   {before_smi.strip()}")
    
    # 4. ç°¡å–®çš„ GPU é‹ç®—æ¸¬è©¦
    print("\n4ï¸âƒ£ åŸ·è¡Œç°¡å–® GPU é‹ç®—:")
    device = torch.device('cuda:0')
    
    # å°çŸ©é™£æ¸¬è©¦
    print("   æ¸¬è©¦ 1: å°çŸ©é™£ä¹˜æ³• (1000x1000)")
    a = torch.randn(1000, 1000).to(device)
    b = torch.randn(1000, 1000).to(device)
    
    torch.cuda.synchronize()
    start = time.time()
    for i in range(100):
        c = torch.matmul(a, b)
    torch.cuda.synchronize()
    elapsed = time.time() - start
    print(f"   âœ… å®Œæˆ 100 æ¬¡é‹ç®—ï¼Œè€—æ™‚: {elapsed:.3f}ç§’")
    
    # æª¢æŸ¥è¨˜æ†¶é«”
    print(f"   è¨˜æ†¶é«”ä½¿ç”¨: {torch.cuda.memory_allocated() / 1024**2:.1f} MB")
    
    # 5. å¤§çŸ©é™£å£“åŠ›æ¸¬è©¦
    print("\n5ï¸âƒ£ GPU å£“åŠ›æ¸¬è©¦ (10ç§’):")
    print("   å‰µå»ºå¤§çŸ©é™£ (8192x8192)...")
    
    try:
        # å‰µå»ºå¤§çŸ©é™£
        size = 8192
        x = torch.randn(size, size, dtype=torch.float32).cuda()
        y = torch.randn(size, size, dtype=torch.float32).cuda()
        
        print(f"   çŸ©é™£å¤§å°: {x.element_size() * x.nelement() / 1024**3:.2f} GB æ¯å€‹")
        print("   é–‹å§‹å£“åŠ›æ¸¬è©¦...")
        
        # å£“åŠ›æ¸¬è©¦ 10 ç§’
        start_time = time.time()
        iteration = 0
        
        while time.time() - start_time < 10:
            # åŸ·è¡ŒçŸ©é™£é‹ç®—
            z = torch.matmul(x, y)
            torch.cuda.synchronize()
            iteration += 1
            
            # æ¯ç§’æª¢æŸ¥ä¸€æ¬¡
            if iteration % 1 == 0:
                # æª¢æŸ¥ GPU ç‹€æ…‹
                gpu_stats = run_command("nvidia-smi --query-gpu=utilization.gpu,temperature.gpu,power.draw --format=csv,noheader")
                print(f"   è¿­ä»£ {iteration}: {gpu_stats.strip()}")
        
        elapsed = time.time() - start_time
        print(f"\n   âœ… å®Œæˆ {iteration} æ¬¡è¿­ä»£ï¼Œç¸½æ™‚é–“: {elapsed:.2f}ç§’")
        print(f"   å¹³å‡é€Ÿåº¦: {iteration/elapsed:.2f} æ¬¡/ç§’")
        
    except RuntimeError as e:
        if "out of memory" in str(e):
            print("   âš ï¸ GPU è¨˜æ†¶é«”ä¸è¶³ï¼Œå˜—è©¦è¼ƒå°çš„çŸ©é™£")
            size = 4096
            x = torch.randn(size, size, dtype=torch.float32).cuda()
            y = torch.randn(size, size, dtype=torch.float32).cuda()
            # é‡è©¦è¼ƒå°çš„æ¸¬è©¦
        else:
            print(f"   âŒ éŒ¯èª¤: {e}")
    
    # 6. æ¸¬è©¦å¾Œæª¢æŸ¥
    print("\n6ï¸âƒ£ æ¸¬è©¦å¾Œç‹€æ…‹:")
    after_smi = run_command("nvidia-smi --query-gpu=utilization.gpu,memory.used,temperature.gpu,power.draw --format=csv,noheader")
    print(f"   {after_smi.strip()}")
    
    # 7. æª¢æŸ¥é€²ç¨‹
    print("\n7ï¸âƒ£ GPU é€²ç¨‹æª¢æŸ¥:")
    processes = run_command("nvidia-smi --query-compute-apps=pid,name,used_memory --format=csv,noheader")
    if processes.strip():
        print(f"   æ‰¾åˆ°é€²ç¨‹:\n{processes}")
    else:
        print("   âŒ æ²’æœ‰æ‰¾åˆ° GPU é€²ç¨‹ï¼")
    
    # 8. é©—è­‰ CUDA è¨­å‚™
    print("\n8ï¸âƒ£ CUDA è¨­å‚™é©—è­‰:")
    print(f"   torch.cuda.current_device(): {torch.cuda.current_device()}")
    print(f"   torch.cuda.get_device_name(): {torch.cuda.get_device_name()}")
    
    # æ¸¬è©¦å¼µé‡æ˜¯å¦çœŸçš„åœ¨ GPU ä¸Š
    test_tensor = torch.randn(100, 100).cuda()
    print(f"   æ¸¬è©¦å¼µé‡è¨­å‚™: {test_tensor.device}")
    print(f"   is_cuda: {test_tensor.is_cuda}")
    
    # 9. æœ€çµ‚è¨ºæ–·
    print("\n" + "=" * 60)
    print("ğŸ” è¨ºæ–·çµæœ:")
    
    # è§£æ GPU ä½¿ç”¨ç‡
    try:
        gpu_util = float(after_smi.split(',')[0].strip().replace('%', ''))
        if gpu_util < 10:
            print("   âŒ GPU ä½¿ç”¨ç‡éä½ï¼å¯èƒ½æ²’æœ‰çœŸæ­£ä½¿ç”¨ GPU")
            print("   å»ºè­°ï¼š")
            print("   1. æª¢æŸ¥ CUDA å®‰è£")
            print("   2. ç¢ºèª PyTorch æ˜¯ GPU ç‰ˆæœ¬")
            print("   3. æª¢æŸ¥é©…å‹•ç¨‹åº")
        else:
            print(f"   âœ… GPU æ­£åœ¨å·¥ä½œ (ä½¿ç”¨ç‡: {gpu_util}%)")
    except:
        print("   âš ï¸ ç„¡æ³•è§£æ GPU ä½¿ç”¨ç‡")
    
    print("=" * 60)

if __name__ == "__main__":
    gpu_stress_test()
